{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## About Dataset\n",
    "\n",
    "Reddit is a discussion website which users can post images and text in a subforum called subreddit which users can discuss about shared contents in comment section. This dataset contains 05/2015 comment submissions from reddit users with 54.504.410 rows and 22 columns.\n",
    "\n",
    "I got my data from kaggle unfornutely this dataset is too big to run on kaggle so I needed to download it.\n",
    "> https://www.kaggle.com/reddit/reddit-comments-may-2015/notebooks\n",
    "\n",
    "If you want a JSON format of this data you can download it from: https://files.pushshift.io/reddit/comments/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Accessing data from sqlite and cleaning it\n",
    "\n",
    "Used this sqlite query to clean the dataset before extracting it to csv because it caused problems while trying to import the data\n",
    "\n",
    "I didn't import authorflaircss_class field because it is not important for our analysis\n",
    "\n",
    "```sqlite\n",
    "create table reddit_2015_05 as\n",
    "select \n",
    "rd.created_utc,\n",
    "rd.ups,\n",
    "rd.subreddit_id,\n",
    "rd.link_id,\n",
    "rd.name,\n",
    "rd.score_hidden,\n",
    "replace(\n",
    "\treplace(\n",
    "\t\treplace(\n",
    "\t\t\t\treplace(\n",
    "\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\t\treplace(rd.author_flair_text,'\\','')\n",
    "\t\t\t\t\t\t,'*','')\n",
    "\t\t\t\t\t,'#','')\n",
    "\t\t\t\t, X'0A', ' ')\n",
    "\t\t,char(13),' ')\n",
    "\t,';','')\n",
    ",'\"','') as author_flair_text,\n",
    "rd.subreddit,\n",
    "rd.id,\n",
    "rd.removal_reason,\n",
    "rd.gilded,\n",
    "rd.downs,\n",
    "rd.archived,\n",
    "rd.author,\n",
    "rd.score,\n",
    "rd.retrieved_on,\n",
    "replace(\n",
    "\treplace(\n",
    "\t\treplace(\n",
    "\t\t\t\treplace(\n",
    "\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\t\treplace(rd.body,'\\','')\n",
    "\t\t\t\t\t\t,'*','')\n",
    "\t\t\t\t\t,'#','')\n",
    "\t\t\t\t, X'0A', ' ')\n",
    "\t\t,char(13),' ')\n",
    "\t,';','')\n",
    ",'\"','') as body,\n",
    "rd.distinguished,\n",
    "rd.edited,\n",
    "rd.controversiality,\n",
    "rd.parent_id\n",
    "from may2015 rd;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Splitting csv data to make it ready for import\n",
    "\n",
    "I needed to split my csv file so I can import it to PostgreSQL because PostgreSQL copy command doesn't support files bigger than 4GB\n",
    "\n",
    "I used [csvsplitter](https://www.erdconcepts.com/dbtoolbox/csvsplitter/csvsplitter.zip) from [erdconcepts](https://www.erdconcepts.com/dbtoolbox.html)\n",
    "\n",
    "Opened up cmd and inserted these lines;\n",
    "\n",
    "```cmd\n",
    "cd C:\\data\\reddit\\csvsplitter\n",
    "\n",
    "CSVSplitter.exe filename=\"C:\\data\\reddit\\reddit_2015_05.csv\" rowcount=5000000\n",
    "```\n",
    "\n",
    "It spliced my csv to 11 files ranging from 1.2GB to 1.5GB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating table in PostgreSQL to import our dataset\n",
    "\n",
    "I created my PostgreSQL table with this query\n",
    "\n",
    "```PostgreSQL\n",
    "CREATE TABLE \"ODS\".\"EXT_REDDIT_COMMENTS\"\n",
    "(\n",
    "    created_utc integer,\n",
    "    ups integer,\n",
    "    subreddit_id text COLLATE pg_catalog.\"default\",\n",
    "    link_id text COLLATE pg_catalog.\"default\",\n",
    "    name text COLLATE pg_catalog.\"default\",\n",
    "    score_hidden text COLLATE pg_catalog.\"default\",\n",
    "    author_flair_text text COLLATE pg_catalog.\"default\",\n",
    "    subreddit text COLLATE pg_catalog.\"default\",\n",
    "    id text COLLATE pg_catalog.\"default\",\n",
    "    removal_reason text COLLATE pg_catalog.\"default\",\n",
    "    gilded integer,\n",
    "    downs integer,\n",
    "    archived text COLLATE pg_catalog.\"default\",\n",
    "    author text COLLATE pg_catalog.\"default\",\n",
    "    score integer,\n",
    "    retrieved_on integer,\n",
    "    body text COLLATE pg_catalog.\"default\",\n",
    "    distinguished text COLLATE pg_catalog.\"default\",\n",
    "    edited text COLLATE pg_catalog.\"default\",\n",
    "    controversiality integer,\n",
    "    parent_id text COLLATE pg_catalog.\"default\"\n",
    ")\n",
    "\n",
    "TABLESPACE pg_default;\n",
    "\n",
    "ALTER TABLE \"ODS\".\"EXT_REDDIT_COMMENTS\"\n",
    "    OWNER to postgres;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing dataset\n",
    "\n",
    "Then used PostgreSQL copy command to import my data;\n",
    "\n",
    "```PostgreSQL\n",
    "SET STATEMENT_TIMEOUT TO 3000000;\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-000.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-001.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-002.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-003.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-004.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-005.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-006.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-007.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-008.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-009.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-010.CSV' DELIMITER ';';\n",
    "\n",
    "COMMIT;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Analyzing our data\n",
    "\n",
    "Original dataset is too big to handle(54.504.410 rows with 33.3GB size) maybe we should check if it is possible to reduce our data while not affecting our analysis\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT \n",
    "COUNT(*)                       \n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2;\n",
    "```\n",
    "This query reduces our data to 54.333.604 rows while removing comments like 'OK' which is not meaningful on its own.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT \n",
    "COUNT(*)                       \n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%');\n",
    "```\n",
    "This would remove 958 bot comments with comment author names contains \"-bot-\" or \"_bot_\", it is not that a huge decrease.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''',''))) LIKE '%im a bot%';\n",
    "```\n",
    "We could also filter comments with \"I'm a bot\" text, this also decreases dataset with 24.918 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]';\n",
    "```\n",
    "This query removes deleted comments which is 3.138.587 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.REMOVAL_REASON) = 0;\n",
    "```\n",
    "We should also remove removed comments which is replaced by removal reason instead of original comments.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %';\n",
    "```\n",
    "We should remove single word comments(1.885.966 rows) because they are not important for our analysis.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %'\n",
    "AND ERS.AUTHOR <> 'AutoModerator';\n",
    "```\n",
    "With this query we remove \"AutoModerator\" user which every subreddit uses it for moderation purposes, It filters 286.444 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT(LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%' OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %'\n",
    "AND ERS.AUTHOR <> 'AutoModerator'\n",
    "AND ERS.AUTHOR <> '[deleted]'\n",
    "```\n",
    "Filtering authors which they deleted their account removes 305.983 rows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Cleaning data\n",
    "\n",
    "Using sql analysis we found out which data to ignore, we must clean data before working on it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Table already exists\nIndex already exists\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import time\n",
    "import math\n",
    "\n",
    "conn_string = 'host={pghost} port={pgport} dbname={pgdatabase} user={pguser} password={pgpassword}'.format(pgdatabase='MEF-BDA-PROD',pguser='postgres',pgpassword='123',pghost='localhost',pgport='5432')\n",
    "conn=psycopg2.connect(conn_string)\n",
    "cur=conn.cursor()\n",
    "\n",
    "def check_if_table_exists(schema,table):\n",
    "    cur.execute(\"select exists(select * from information_schema.tables where table_schema='{schema}' AND table_name='{table}')\".format(schema=schema, table=table))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def check_if_index_exists(index):\n",
    "    cur.execute(\"SELECT EXISTS(SELECT * FROM PG_CLASS WHERE relname = '{index}')\".format(index=index))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "if(check_if_table_exists('EDW','DWH_REDDIT_COMMENTS')):\n",
    "    print('Table already exists')   \n",
    "else:\n",
    "    start_time = time.time()\n",
    "    cur.execute('set time zone UTC;')\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE \"EDW\".\"DWH_REDDIT_COMMENTS\" AS \n",
    "    SELECT\n",
    "    ROW_NUMBER() OVER (ORDER BY ERS.ID) AS ID,\n",
    "    TO_TIMESTAMP(GREATEST(ERS.CREATED_UTC ,CAST(ERS.EDITED AS INTEGER))) AS DATE,\n",
    "    ERS.SUBREDDIT,\n",
    "    ERS.AUTHOR,\n",
    "    ERS.AUTHOR_FLAIR_TEXT,\n",
    "    ERS.SCORE,\n",
    "    ERS.BODY AS COMMENT\n",
    "    FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "    WHERE 1=1\n",
    "    AND LENGTH(ERS.BODY) > 2\n",
    "    AND NOT(LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%' OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "    AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "    AND ERS.BODY <> '[deleted]'\n",
    "    AND LENGTH(ERS.removal_reason) = 0\n",
    "    AND ERS.BODY LIKE '% %'\n",
    "    AND ERS.AUTHOR <> 'AutoModerator'\n",
    "    AND ERS.AUTHOR <> '[deleted]';\n",
    "    \"\"\")\n",
    "    cur.execute('COMMIT;')\n",
    "    print(\"Table created in {execute_time} seconds\".format(execute_time=math.trunc(time.time()-start_time)))\n",
    "\n",
    "if(check_if_index_exists('IDX_DWH_REDDIT_COMMENTS#01')):\n",
    "    print('Index already exists')\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX \"IDX_DWH_REDDIT_COMMENTS#01\" \n",
    "    ON \"EDW\".\"DWH_REDDIT_COMMENTS\" USING BTREE(\n",
    "        \"id\" ASC NULLS LAST,\n",
    "        \"date\" ASC NULLS LAST\n",
    "    )\n",
    "    TABLESPACE PG_DEFAULT;\n",
    "    \"\"\")\n",
    "    cur.execute('COMMIT;')\n",
    "    print(\"Index created in {execute_time} seconds\".format(execute_time=math.trunc(time.time()-start_time)))"
   ]
  },
  {
   "source": [
    "1. We filtered our data and transformed epoch date to readable date and added numeric id to work our data with batch processing.\n",
    "    It reduced our row count 54.504.410(with 33.3GB) to 48.690.746(with 24.5GB) with 11% reduction in rows and 27% reduction in size.\n",
    "\n",
    "2. Added index to increase our read speed from table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                    key    max_price\n",
       "0                                000mah    93.374000\n",
       "1                               0g03674   209.851250\n",
       "2                              1000watt   998.326667\n",
       "3                                 1000x   191.996667\n",
       "4                              100400mm  1574.278571\n",
       "...                                 ...          ...\n",
       "2926                             zubehr    58.730000\n",
       "2927                              zubie    99.990000\n",
       "2928  zuxbbweveteztffywrybecztecqtezfbc  1799.970000\n",
       "2929                     zxqyvbwuwyydcq    18.158000\n",
       "2930      zxsqvcbvvywsrwuaasvczufystyyy    75.882857\n",
       "\n",
       "[2931 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>max_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000mah</td>\n      <td>93.374000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0g03674</td>\n      <td>209.851250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000watt</td>\n      <td>998.326667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000x</td>\n      <td>191.996667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100400mm</td>\n      <td>1574.278571</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2926</th>\n      <td>zubehr</td>\n      <td>58.730000</td>\n    </tr>\n    <tr>\n      <th>2927</th>\n      <td>zubie</td>\n      <td>99.990000</td>\n    </tr>\n    <tr>\n      <th>2928</th>\n      <td>zuxbbweveteztffywrybecztecqtezfbc</td>\n      <td>1799.970000</td>\n    </tr>\n    <tr>\n      <th>2929</th>\n      <td>zxqyvbwuwyydcq</td>\n      <td>18.158000</td>\n    </tr>\n    <tr>\n      <th>2930</th>\n      <td>zxsqvcbvvywsrwuaasvczufystyyy</td>\n      <td>75.882857</td>\n    </tr>\n  </tbody>\n</table>\n<p>2931 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import fsspec\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "from pandas import DataFrame\n",
    "\n",
    "def download_file_if_not_exists(file_url,file_name):\n",
    "    start_time = math.trunc(time.time())\n",
    "    if(os.path.exists(file_name) and os.stat(file_name).st_size==0):\n",
    "        os.remove(file_name)\n",
    "    if(not(os.path.exists(file_name))):\n",
    "        urllib.request.urlretrieve(file_url,file_name)\n",
    "        with open(file_name, 'r+', errors='ignore', encoding=\"utf-8\") as f:\n",
    "            file_text = f.read()\n",
    "            file_text = re.sub(r'\"[^\"]*\"', lambda m: m.group(0).replace(',', ' '), file_text).replace('\\\\','').replace('\"','').replace(\"'\",'')\n",
    "            f.seek(0)\n",
    "            f.write(file_text)\n",
    "            f.truncate()\n",
    "    end_time = math.trunc(time.time())\n",
    "    if(start_time!=end_time):\n",
    "        print(\"File downloaded and cleaned in {execute_time} seconds\".format(execute_time=end_time-start_time))\n",
    "\n",
    "file_name = \"DatafinitiElectronicsProductsPricingData.csv\"\n",
    "file_url = \"https://query.data.world/s/n7byb65oqj47oro2btcqqyas62zclv\"\n",
    "download_file_if_not_exists(file_url,file_name)\n",
    "\n",
    "file_name = \"electronic_products_pricing_df.xlsx\"\n",
    "if(os.path.exists(file_name)):\n",
    "    electronic_products_pricing_df = pd.read_excel(file_name)\n",
    "else:\n",
    "    electronic_products_pricing_df = pd.read_csv(\"DatafinitiElectronicsProductsPricingData.csv\", encoding=\"utf-8\")\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.loc[:, ~electronic_products_pricing_df.columns.str.contains('^Unnamed')]\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df[electronic_products_pricing_df[\"prices.currency\"] == \"USD\"]\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df[[\"name\",\"brand\",\"categories\",\"prices.amountMax\"]]\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.groupby([\"name\",\"brand\",\"categories\"]).mean()\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.reset_index()\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.rename(columns = {\"prices.amountMax\":\"average_price\"})\n",
    "    electronic_products_pricing_df[\"keys\"] =  electronic_products_pricing_df[[\"name\",\"brand\",\"categories\"]].agg(' '.join, axis=1, ).str.lower()\n",
    "    electronic_products_pricing_df.to_excel(file_name, engine='xlsxwriter')\n",
    "\n",
    "file_name = \"keys_pricing_df.xlsx\"\n",
    "if(os.path.exists(file_name)):\n",
    "    keys_pricing_df = pd.read_excel(file_name,index_col=0)\n",
    "else:\n",
    "    keys_pricing_df = DataFrame(electronic_products_pricing_df[\"keys\"].replace('&','').str.split(' ').tolist(), index=electronic_products_pricing_df[\"average_price\"]).stack()\n",
    "    keys_pricing_df = keys_pricing_df.reset_index()\n",
    "    keys_pricing_df.columns = [\"average_price\",\"level\",\"key\"]\n",
    "    keys_pricing_df = keys_pricing_df[keys_pricing_df[\"key\"] != \"&\"]\n",
    "    keys_pricing_df = keys_pricing_df[[\"key\",\"average_price\"]]\n",
    "    keys_pricing_df[\"key\"] = keys_pricing_df[\"key\"].str.replace(r'[^A-Za-z0-9]+','')\n",
    "    keys_pricing_df[\"key\"] = keys_pricing_df[\"key\"][~keys_pricing_df[\"key\"].str.isnumeric()]\n",
    "    keys_pricing_df = keys_pricing_df[keys_pricing_df[\"key\"] != \"\"] #Used this after realizing dropna doesn't work for some reason\n",
    "    keys_pricing_df = keys_pricing_df.dropna()\n",
    "    keys_pricing_df = keys_pricing_df.groupby([\"key\"]).max()\n",
    "    keys_pricing_df = keys_pricing_df.reset_index()\n",
    "    keys_pricing_df = keys_pricing_df.rename(columns = {\"average_price\":\"max_price\"})\n",
    "    keys_pricing_df.to_excel(file_name, engine='xlsxwriter')\n",
    "\n",
    "keys_pricing_df"
   ]
  },
  {
   "source": [
    "We are getting some price and key data to analize how much we would gain if we presented adds based on their comments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Removing stopwords and using Natural Language Processing while Detecting Language\n",
    "\n",
    "Stopwords are commonly used words that must be not stored because they do not give any significant value to analyze. We are going to remove them from our table so we can lower our datasize and proccess our data faster.\n",
    "\n",
    "We must test which libraries to use and how to use them;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "String 'Testing this value' returns: ('ENGLISH', 'en', 95, 1077.0)\n\nString 'Bu değeri deniyoruz' returns: ('TURKISH', 'tr', 95, 1706.0)\n\nString 'Test text' returns: ('Unknown', 'un', 0, 0.0)\n\n"
     ]
    }
   ],
   "source": [
    "import pycld2\n",
    "\n",
    "value = 'Testing this value'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0])) #This will return the language with highest confidence score.\n",
    "value = 'Bu değeri deniyoruz'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0]))\n",
    "value = 'Test text'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0]))"
   ]
  },
  {
   "source": [
    "import nltk\n",
    "import pycld2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "sql_command = 'SELECT * FROM \"{schema}\".\"{table}\" WHERE ID = 15;'.format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "print(\"Original comment:\\n{comment}\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stop)]))\n",
    "\n",
    "print(\"Comment after removing stopwords:\\n{comment}\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "start_time = time.time()\n",
    "cur.execute(\"\"\"\n",
    "UPDATE \"EDW\".\"DWH_REDDIT_COMMENTS\"\n",
    "SET \"comment\" = %(comment)s\n",
    "WHERE \"id\" = %(id)s\n",
    "\"\"\", {'comment': str(df['comment'][0]), 'id': int(df['id'][0])})\n",
    "\n",
    "print(\"Updated record(s) in {execute_time} seconds\\n\".format(execute_time=math.trunc(time.time()-start_time)))\n",
    "\n",
    "sql_command = 'SELECT * FROM \"{schema}\".\"{table}\" WHERE ID = 15;'.format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "cur.execute('ROLLBACK;')\n",
    "print(\"Comment in table:\\n{comment}\\n\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "df['comment_language_code'] = pycld2.detect(str(df['comment']))[2][0][1]\n",
    "df['comment_language'] = pycld2.detect(str(df['comment']))[2][0][0].lower().replace('unknown','english')\n",
    "\n",
    "df\n",
    "#Since we are testing here we didn't commit to database so our changes are going to be rolled back after our session dies. It will be used after completing our test.\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original comment:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\nComment after removing stopwords:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\nUpdated record(s) in 0 seconds\n\nComment in table:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                      date   subreddit      author author_flair_text  \\\n",
       "0  15 2015-05-01 00:00:00+00:00  offmychest  Zekkystyle                     \n",
       "\n",
       "   score                                            comment  \\\n",
       "0     14  fault think that. hey helps well. least talk i...   \n",
       "\n",
       "  comment_language_code comment_language  \n",
       "0                    en          english  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>subreddit</th>\n      <th>author</th>\n      <th>author_flair_text</th>\n      <th>score</th>\n      <th>comment</th>\n      <th>comment_language_code</th>\n      <th>comment_language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>2015-05-01 00:00:00+00:00</td>\n      <td>offmychest</td>\n      <td>Zekkystyle</td>\n      <td></td>\n      <td>14</td>\n      <td>fault think that. hey helps well. least talk i...</td>\n      <td>en</td>\n      <td>english</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "source": [
    "It reduces our data well and it still makes sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           id                      date             subreddit  \\\n",
       "0    30545495 2015-05-20 12:13:05+00:00                Turkey   \n",
       "1    30545496 2015-05-20 12:13:06+00:00                  cats   \n",
       "2    30545497 2015-05-20 12:13:06+00:00              Database   \n",
       "3    30545498 2015-05-20 12:13:06+00:00         unitedkingdom   \n",
       "4    30545499 2015-05-20 12:13:06+00:00                skyrim   \n",
       "..        ...                       ...                   ...   \n",
       "96   30545591 2015-05-20 12:13:12+00:00                Coffee   \n",
       "97   30545592 2015-05-20 12:13:12+00:00                  news   \n",
       "98   30545593 2015-05-20 12:13:12+00:00                  h1z1   \n",
       "99   30545594 2015-05-20 12:13:12+00:00  BeforeNAfterAdoption   \n",
       "100  30545595 2015-05-20 12:13:12+00:00               FlashTV   \n",
       "\n",
       "                  author         author_flair_text  score  \\\n",
       "0               shiguree                                1   \n",
       "1               lovechip                               10   \n",
       "2                cojajoc                                7   \n",
       "3                JamDunc  ex-Yorkshire now Sverige     11   \n",
       "4         Open_Info_of94                                2   \n",
       "..                   ...                       ...    ...   \n",
       "96    Italian_Not_Jewish                                1   \n",
       "97        PrivateHazzard                                1   \n",
       "98               Tenetri                                1   \n",
       "99            PantsGiver                               14   \n",
       "100  SuperBattleFranky37                                3   \n",
       "\n",
       "                                               comment comment_language_code  \\\n",
       "0    İnsana insan gibi yaşama fırsatı verildiğinde,...                    tr   \n",
       "1    This looks just like a cat I used to know (tha...                    en   \n",
       "2    partitioning the table would be useful in this...                    en   \n",
       "3    Well Sweden's system is open and not run by th...                    en   \n",
       "4    Lol he said role play to hard. That's not a re...                    en   \n",
       "..                                                 ...                   ...   \n",
       "96   I've had the same issue, brought the French Pr...                    en   \n",
       "97   This logic is incorrect... You don't need a wa...                    en   \n",
       "98   but, i've gotten so many streamers with &lt5 v...                    en   \n",
       "99   Being inside cats is really what helps. They c...                    en   \n",
       "100  Now I'm curious as to Cisco's source for the D...                    en   \n",
       "\n",
       "    comment_language                          comment_without_stopwords  \n",
       "0            turkish  i nsana insan yaşama fırsatı verildiğinde şimd...  \n",
       "1            english  looks like cat used know  that sounds like wei...  \n",
       "2            english  partitioning table would useful   partition da...  \n",
       "3            english  well sweden s system open run government   alt...  \n",
       "4            english        lol said role play hard  that s real thing   \n",
       "..               ...                                                ...  \n",
       "96           english  i ve issue  brought french press help hard cle...  \n",
       "97           english  logic incorrect    need war crime    realize u...  \n",
       "98           english  but  i ve gotten many streamers  lt5 viewers b...  \n",
       "99           english  inside cats really helps  also live fiv  cats ...  \n",
       "100          english  i m curious cisco s source dna tested crime sc...  \n",
       "\n",
       "[101 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>subreddit</th>\n      <th>author</th>\n      <th>author_flair_text</th>\n      <th>score</th>\n      <th>comment</th>\n      <th>comment_language_code</th>\n      <th>comment_language</th>\n      <th>comment_without_stopwords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30545495</td>\n      <td>2015-05-20 12:13:05+00:00</td>\n      <td>Turkey</td>\n      <td>shiguree</td>\n      <td></td>\n      <td>1</td>\n      <td>İnsana insan gibi yaşama fırsatı verildiğinde,...</td>\n      <td>tr</td>\n      <td>turkish</td>\n      <td>i nsana insan yaşama fırsatı verildiğinde şimd...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30545496</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>cats</td>\n      <td>lovechip</td>\n      <td></td>\n      <td>10</td>\n      <td>This looks just like a cat I used to know (tha...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>looks like cat used know  that sounds like wei...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30545497</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>Database</td>\n      <td>cojajoc</td>\n      <td></td>\n      <td>7</td>\n      <td>partitioning the table would be useful in this...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>partitioning table would useful   partition da...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30545498</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>unitedkingdom</td>\n      <td>JamDunc</td>\n      <td>ex-Yorkshire now Sverige</td>\n      <td>11</td>\n      <td>Well Sweden's system is open and not run by th...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>well sweden s system open run government   alt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30545499</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>skyrim</td>\n      <td>Open_Info_of94</td>\n      <td></td>\n      <td>2</td>\n      <td>Lol he said role play to hard. That's not a re...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>lol said role play hard  that s real thing</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>30545591</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>Coffee</td>\n      <td>Italian_Not_Jewish</td>\n      <td></td>\n      <td>1</td>\n      <td>I've had the same issue, brought the French Pr...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>i ve issue  brought french press help hard cle...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>30545592</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>news</td>\n      <td>PrivateHazzard</td>\n      <td></td>\n      <td>1</td>\n      <td>This logic is incorrect... You don't need a wa...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>logic incorrect    need war crime    realize u...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>30545593</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>h1z1</td>\n      <td>Tenetri</td>\n      <td></td>\n      <td>1</td>\n      <td>but, i've gotten so many streamers with &amp;lt5 v...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>but  i ve gotten many streamers  lt5 viewers b...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>30545594</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>BeforeNAfterAdoption</td>\n      <td>PantsGiver</td>\n      <td></td>\n      <td>14</td>\n      <td>Being inside cats is really what helps. They c...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>inside cats really helps  also live fiv  cats ...</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>30545595</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>FlashTV</td>\n      <td>SuperBattleFranky37</td>\n      <td></td>\n      <td>3</td>\n      <td>Now I'm curious as to Cisco's source for the D...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>i m curious cisco s source dna tested crime sc...</td>\n    </tr>\n  </tbody>\n</table>\n<p>101 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import nltk\n",
    "import pycld2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "start_row = 1\n",
    "row_per_loop = 1000\n",
    "end_row = row_per_loop\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "sql_command = \"\"\"SELECT * FROM \"{schema}\".\"{table}\" WHERE ID BETWEEN 30545495 AND 30545595;\"\"\".format(schema='EDW', table='DWH_REDDIT_COMMENTS', start_row=start_row, end_row=end_row)\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "\n",
    "#30545495 AND 30545595\n",
    "\n",
    "df['comment_language_code'] = df['comment'].str.replace(r'[^A-Za-z0-9]+',' ').apply(pycld2.detect).apply(lambda x: x[2][0][1])\n",
    "df['comment_language'] = df['comment'].str.replace(r'[^A-Za-z0-9]+',' ').apply(pycld2.detect).apply(lambda x: x[2][0][0]).str.lower()\n",
    "\n",
    "def remove_stopwords(text,text_language):\n",
    "    text_without_stopwords = ' '\n",
    "    try:\n",
    "        text_without_stopwords = text_without_stopwords.join([word for word in text.lower().split() if word not in (stopwords.words(text_language))])\n",
    "    except:\n",
    "        text_without_stopwords = text_without_stopwords.join([word for word in text.lower().split() if word not in (stopwords.words('english'))])\n",
    "    \n",
    "    text_without_stopwords = re.sub(\"[\\W]\",\" \",text_without_stopwords,re.UNICODE)\n",
    "    #text_without_stopwords = re.sub(\"[^\\p{L} 0-9]\",\" \",text_without_stopwords,re.UNICODE)\n",
    "    return text_without_stopwords\n",
    "\n",
    "df['comment_without_stopwords'] = df.apply(lambda x: remove_stopwords(x['comment'], x['comment_language']), axis=1)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "source": [
    "## Sources:\n",
    "\n",
    " 1. [About Reddit](https://en.wikipedia.org/wiki/Reddit)\n",
    "\n",
    " 2. [Data source](https://www.kaggle.com/reddit/reddit-comments-may-2015/notebooks)\n",
    "\n",
    " 3. [Checking if a table exist with psycopg2 on postgreSQL](https://stackoverflow.com/questions/1874113/checking-if-a-postgresql-table-exists-under-python-and-probably-psycopg2)\n",
    "\n",
    " 4. [Using current time in UTC as default value in PostgreSQL. This is important because date is utc in the data](https://stackoverflow.com/questions/16609724/using-current-time-in-utc-as-default-value-in-postgresql)\n",
    "\n",
    " 5. [Creating multicolumn index on PostgreSQL](https://www.postgresql.org/docs/9.6/indexes-multicolumn.html)\n",
    "\n",
    " 6. [Checking if index exist](https://stackoverflow.com/questions/45983169/checking-for-existence-of-index-in-postgresql)\n",
    "\n",
    " 7. [How to execute start time and end time in python](https://www.codegrepper.com/code-examples/delphi/how+to+execute+from+start+time+to+end+time+in+python)\n",
    "\n",
    " 8. [Truncating numbers in python](https://www.w3schools.com/python/ref_math_trunc.asp)\n",
    "\n",
    " 9. [Removing stopwords](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe)\n",
    "\n",
    " 10. [Prevent SQL Injection in Python](https://realpython.com/prevent-python-sql-injection/)\n",
    "\n",
    " 11. [Preventing SQL Injection resulted errors but It needed to be done, data type conversation is the key here](https://stackoverflow.com/questions/39564755/programmingerror-psycopg2-programmingerror-cant-adapt-type-numpy-ndarray)\n",
    "\n",
    " 12. [About stopwords](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n",
    "\n",
    " 13. [How to detect language](https://github.com/aboSamoor/pycld2)\n",
    "\n",
    " 14. [Increasing timeout while installing new packages](https://stackoverflow.com/questions/43298872/how-to-solve-readtimeouterror-httpsconnectionpoolhost-pypi-python-org-port)\n",
    "\n",
    " 15. [PyCld2 is only works in linux systems](https://www.lfd.uci.edu/~gohlke/pythonlibs/)\n",
    "\n",
    " 16. [Replacing text to change unknown values to english](https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-column-of-a-pandas-dataframe)\n",
    "\n",
    " 17. [Electronic Products and Pricing Data](https://data.world/datafiniti/electronic-products-and-pricing-data/workspace/file?filename=DatafinitiElectronicsProductsPricingData.csv)\n",
    "\n",
    " 18. [Remove all commas between quotes](https://stackoverflow.com/questions/38336518/remove-all-commas-between-quotes)\n",
    "\n",
    " 19. [Check if file exist in directory](https://stackoverflow.com/questions/28144529/how-to-check-if-file-already-exists-if-not-download-on-python)\n",
    "\n",
    " 20. [Download files](https://stackabuse.com/download-files-with-python/)\n",
    "\n",
    " 21. [Deleting empty files](https://stackoverflow.com/questions/48046729/delete-empty-files)\n",
    "\n",
    " 22. [Removing unnamed columns](https://stackoverflow.com/questions/43983622/remove-unnamed-columns-in-pandas-dataframe)\n",
    "\n",
    " 23. [Split (explode) pandas dataframe string entry to separate rows](https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows)\n",
    "\n",
    " 24. [Dropping numeric values](https://stackoverflow.com/questions/48636170/dropping-numeric-rows-from-dataframe-in-python)\n",
    "\n",
    " 25. [Join function](https://www.geeksforgeeks.org/join-function-python/)\n",
    "\n",
    " 26. [Apply function with two arguments to columns](https://stackoverflow.com/questions/34279378/python-pandas-apply-function-with-two-arguments-to-columns)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}