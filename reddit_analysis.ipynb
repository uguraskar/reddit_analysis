{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## About Dataset\n",
    "\n",
    "Reddit is a discussion website which users can post images and text in a subforum called subreddit which users can discuss about shared contents in comment section. This dataset contains 05/2015 comment submissions from reddit users with 54.504.410 rows and 22 columns.\n",
    "\n",
    "I got my data from kaggle unfornutely this dataset is too big to run on kaggle so I needed to download it.\n",
    "> https://www.kaggle.com/reddit/reddit-comments-may-2015/notebooks\n",
    "\n",
    "If you want a JSON format of this data you can download it from: https://files.pushshift.io/reddit/comments/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Accessing data from sqlite and cleaning it\n",
    "\n",
    "Used this sqlite query to clean the dataset before extracting it to csv because it caused problems while trying to import the data\n",
    "\n",
    "I didn't import authorflaircss_class field because it is not important for our analysis\n",
    "\n",
    "```sqlite\n",
    "create table reddit_2015_05 as\n",
    "select \n",
    "rd.created_utc,\n",
    "rd.ups,\n",
    "rd.subreddit_id,\n",
    "rd.link_id,\n",
    "rd.name,\n",
    "rd.score_hidden,\n",
    "replace(\n",
    "\treplace(\n",
    "\t\treplace(\n",
    "\t\t\t\treplace(\n",
    "\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\t\treplace(rd.author_flair_text,'\\','')\n",
    "\t\t\t\t\t\t,'*','')\n",
    "\t\t\t\t\t,'#','')\n",
    "\t\t\t\t, X'0A', ' ')\n",
    "\t\t,char(13),' ')\n",
    "\t,';','')\n",
    ",'\"','') as author_flair_text,\n",
    "rd.subreddit,\n",
    "rd.id,\n",
    "rd.removal_reason,\n",
    "rd.gilded,\n",
    "rd.downs,\n",
    "rd.archived,\n",
    "rd.author,\n",
    "rd.score,\n",
    "rd.retrieved_on,\n",
    "replace(\n",
    "\treplace(\n",
    "\t\treplace(\n",
    "\t\t\t\treplace(\n",
    "\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\t\treplace(rd.body,'\\','')\n",
    "\t\t\t\t\t\t,'*','')\n",
    "\t\t\t\t\t,'#','')\n",
    "\t\t\t\t, X'0A', ' ')\n",
    "\t\t,char(13),' ')\n",
    "\t,';','')\n",
    ",'\"','') as body,\n",
    "rd.distinguished,\n",
    "rd.edited,\n",
    "rd.controversiality,\n",
    "rd.parent_id\n",
    "from may2015 rd;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Splitting csv data to make it ready for import\n",
    "\n",
    "I needed to split my csv file so I can import it to PostgreSQL because PostgreSQL copy command doesn't support files bigger than 4GB\n",
    "\n",
    "I used [csvsplitter](https://www.erdconcepts.com/dbtoolbox/csvsplitter/csvsplitter.zip) from [erdconcepts](https://www.erdconcepts.com/dbtoolbox.html)\n",
    "\n",
    "Opened up cmd and inserted these lines;\n",
    "\n",
    "```cmd\n",
    "cd C:\\data\\reddit\\csvsplitter\n",
    "\n",
    "CSVSplitter.exe filename=\"C:\\data\\reddit\\reddit_2015_05.csv\" rowcount=5000000\n",
    "```\n",
    "\n",
    "It spliced my csv to 11 files ranging from 1.2GB to 1.5GB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating table in PostgreSQL to import our dataset\n",
    "\n",
    "I created my PostgreSQL table with this query\n",
    "\n",
    "```PostgreSQL\n",
    "CREATE TABLE \"ODS\".\"EXT_REDDIT_COMMENTS\"\n",
    "(\n",
    "    created_utc integer,\n",
    "    ups integer,\n",
    "    subreddit_id text COLLATE pg_catalog.\"default\",\n",
    "    link_id text COLLATE pg_catalog.\"default\",\n",
    "    name text COLLATE pg_catalog.\"default\",\n",
    "    score_hidden text COLLATE pg_catalog.\"default\",\n",
    "    author_flair_text text COLLATE pg_catalog.\"default\",\n",
    "    subreddit text COLLATE pg_catalog.\"default\",\n",
    "    id text COLLATE pg_catalog.\"default\",\n",
    "    removal_reason text COLLATE pg_catalog.\"default\",\n",
    "    gilded integer,\n",
    "    downs integer,\n",
    "    archived text COLLATE pg_catalog.\"default\",\n",
    "    author text COLLATE pg_catalog.\"default\",\n",
    "    score integer,\n",
    "    retrieved_on integer,\n",
    "    body text COLLATE pg_catalog.\"default\",\n",
    "    distinguished text COLLATE pg_catalog.\"default\",\n",
    "    edited text COLLATE pg_catalog.\"default\",\n",
    "    controversiality integer,\n",
    "    parent_id text COLLATE pg_catalog.\"default\"\n",
    ")\n",
    "\n",
    "TABLESPACE pg_default;\n",
    "\n",
    "ALTER TABLE \"ODS\".\"EXT_REDDIT_COMMENTS\"\n",
    "    OWNER to postgres;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing dataset\n",
    "\n",
    "Then used PostgreSQL copy command to import my data;\n",
    "\n",
    "```PostgreSQL\n",
    "SET STATEMENT_TIMEOUT TO 3000000;\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-000.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-001.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-002.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-003.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-004.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-005.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-006.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-007.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-008.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-009.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-010.CSV' DELIMITER ';';\n",
    "\n",
    "COMMIT;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Analyzing our data\n",
    "\n",
    "Original dataset is too big to handle(54.504.410 rows with 33.3GB size) maybe we should check if it is possible to reduce our data while not affecting our analysis\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT \n",
    "COUNT(*)                       \n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2;\n",
    "```\n",
    "This query reduces our data to 54.333.604 rows while removing comments like 'OK' which is not meaningful on its own.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT \n",
    "COUNT(*)                       \n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%');\n",
    "```\n",
    "This would remove 958 bot comments with comment author names contains \"-bot-\" or \"_bot_\", it is not that a huge decrease.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''',''))) LIKE '%im a bot%';\n",
    "```\n",
    "We could also filter comments with \"I'm a bot\" text, this also decreases dataset with 24.918 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]';\n",
    "```\n",
    "This query removes deleted comments which is 3.138.587 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.REMOVAL_REASON) = 0;\n",
    "```\n",
    "We should also remove removed comments which is replaced by removal reason instead of original comments.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %';\n",
    "```\n",
    "We should remove single word comments(1.885.966 rows) because they are not important for our analysis.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %'\n",
    "AND ERS.AUTHOR <> 'AutoModerator';\n",
    "```\n",
    "With this query we remove \"AutoModerator\" user which every subreddit uses it for moderation purposes, It filters 286.444 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT(LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%' OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %'\n",
    "AND ERS.AUTHOR <> 'AutoModerator'\n",
    "AND ERS.AUTHOR <> '[deleted]'\n",
    "```\n",
    "Filtering authors which they deleted their account removes 305.983 rows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Cleaning data\n",
    "\n",
    "Using sql analysis we found out which data to ignore, we must clean data before working on it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Table already exists\nIndex already exists\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import time\n",
    "import math\n",
    "\n",
    "conn_string = 'host={pghost} port={pgport} dbname={pgdatabase} user={pguser} password={pgpassword}'.format(pgdatabase='MEF-BDA-PROD',pguser='postgres',pgpassword='123',pghost='localhost',pgport='5432')\n",
    "conn=psycopg2.connect(conn_string)\n",
    "cur=conn.cursor()\n",
    "\n",
    "def check_if_table_exists(schema,table):\n",
    "    cur.execute(\"select exists(select * from information_schema.tables where table_schema='{schema}' AND table_name='{table}')\".format(schema=schema, table=table))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def check_if_index_exists(index):\n",
    "    cur.execute(\"SELECT EXISTS(SELECT * FROM PG_CLASS WHERE relname = '{index}')\".format(index=index))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "if(check_if_table_exists('EDW','DWH_REDDIT_COMMENTS')):\n",
    "    print('Table already exists')   \n",
    "else:\n",
    "    start_time = time.time()\n",
    "    cur.execute('set time zone UTC;')\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE \"EDW\".\"DWH_REDDIT_COMMENTS\" AS \n",
    "    SELECT\n",
    "    ROW_NUMBER() OVER (ORDER BY ERS.ID) AS ID,\n",
    "    TO_TIMESTAMP(GREATEST(ERS.CREATED_UTC ,CAST(ERS.EDITED AS INTEGER))) AS DATE,\n",
    "    ERS.SUBREDDIT,\n",
    "    ERS.AUTHOR,\n",
    "    ERS.AUTHOR_FLAIR_TEXT,\n",
    "    ERS.SCORE,\n",
    "    ERS.BODY AS COMMENT\n",
    "    FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "    WHERE 1=1\n",
    "    AND LENGTH(ERS.BODY) > 2\n",
    "    AND NOT(LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%' OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "    AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "    AND ERS.BODY <> '[deleted]'\n",
    "    AND LENGTH(ERS.removal_reason) = 0\n",
    "    AND ERS.BODY LIKE '% %'\n",
    "    AND ERS.AUTHOR <> 'AutoModerator'\n",
    "    AND ERS.AUTHOR <> '[deleted]';\n",
    "    \"\"\")\n",
    "    cur.execute('COMMIT;')\n",
    "    print(\"Table created in {execute_time} seconds\".format(execute_time=math.trunc(time.time()-start_time)))\n",
    "\n",
    "if(check_if_index_exists('IDX_DWH_REDDIT_COMMENTS#01')):\n",
    "    print('Index already exists')\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX \"IDX_DWH_REDDIT_COMMENTS#01\" \n",
    "    ON \"EDW\".\"DWH_REDDIT_COMMENTS\" USING BTREE(\n",
    "        \"id\" ASC NULLS LAST,\n",
    "        \"date\" ASC NULLS LAST\n",
    "    )\n",
    "    TABLESPACE PG_DEFAULT;\n",
    "    \"\"\")\n",
    "    cur.execute('COMMIT;')\n",
    "    print(\"Index created in {execute_time} seconds\".format(execute_time=math.trunc(time.time()-start_time)))"
   ]
  },
  {
   "source": [
    "1. We filtered our data and transformed epoch date to readable date and added numeric id to work our data with batch processing.\n",
    "    It reduced our row count 54.504.410(with 33.3GB) to 48.690.746(with 24.5GB) with 11% reduction in rows and 27% reduction in size.\n",
    "\n",
    "2. Added index to increase our read speed from table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import fsspec\n",
    "\n",
    "file_name = \"DatafinitiElectronicsProductsPricingData.csv\"\n",
    "file_url = \"https://query.data.world/s/n7byb65oqj47oro2btcqqyas62zclv\"\n",
    "\n",
    "def download_file_if_not_exists(file_url,file_name):\n",
    "    start_time = math.trunc(time.time())\n",
    "    if(os.path.exists(file_name) and os.stat(file_name).st_size==0):\n",
    "        os.remove(file_name)\n",
    "    if(not(os.path.exists(file_name))):\n",
    "        urllib.request.urlretrieve(file_url,file_name)\n",
    "        with open(file_name, 'r+', errors='ignore', encoding=\"utf-8\") as f:\n",
    "            file_text = f.read()\n",
    "            file_text = re.sub(r'\"[^\"]*\"', lambda m: m.group(0).replace(',', ' '), file_text).replace('\\\\','').replace('\"','').replace(\"'\",'')\n",
    "            f.seek(0)\n",
    "            f.write(file_text)\n",
    "            f.truncate()\n",
    "    end_time = math.trunc(time.time())\n",
    "    if(start_time!=end_time):\n",
    "        print(\"File downloaded and cleaned in {execute_time} seconds\".format(execute_time=end_time-start_time))\n",
    "\n",
    "download_file_if_not_exists(file_url,file_name)\n",
    "\n",
    "electronic_products_pricing_df = pd.read_csv(file_name, encoding=\"utf-8\")\n",
    "\n",
    "electronic_products_pricing_df = electronic_products_pricing_df.loc[:, ~electronic_products_pricing_df.columns.str.contains('^Unnamed')]\n",
    "#electronic_products_pricing_df = electronic_products_pricing_df.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "electronic_products_pricing_df.to_excel('electronic_products_pricing_df.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "source": [
    "## Removing stopwords and using Natural Language Processing while Detecting Language\n",
    "\n",
    "Stopwords are commonly used words that must be not stored because they do not give any significant value to analyze. We are going to remove them from our table so we can lower our datasize and proccess our data faster.\n",
    "\n",
    "We must test which libraries to use and how to use them;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "String 'Testing this value' returns: ('ENGLISH', 'en', 95, 1077.0)\n\nString 'Bu değeri deniyoruz' returns: ('TURKISH', 'tr', 95, 1706.0)\n\nString 'Test text' returns: ('Unknown', 'un', 0, 0.0)\n\n"
     ]
    }
   ],
   "source": [
    "import pycld2\n",
    "\n",
    "value = 'Testing this value'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0])) #This will return the language with highest confidence score.\n",
    "value = 'Bu değeri deniyoruz'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0]))\n",
    "value = 'Test text'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0]))"
   ]
  },
  {
   "source": [
    "import nltk\n",
    "import pycld2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "sql_command = 'SELECT * FROM \"{schema}\".\"{table}\" WHERE ID = 15;'.format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "print(\"Original comment:\\n{comment}\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stop)]))\n",
    "\n",
    "print(\"Comment after removing stopwords:\\n{comment}\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "start_time = time.time()\n",
    "cur.execute(\"\"\"\n",
    "UPDATE \"EDW\".\"DWH_REDDIT_COMMENTS\"\n",
    "SET \"comment\" = %(comment)s\n",
    "WHERE \"id\" = %(id)s\n",
    "\"\"\", {'comment': str(df['comment'][0]), 'id': int(df['id'][0])})\n",
    "\n",
    "print(\"Updated record(s) in {execute_time} seconds\\n\".format(execute_time=math.trunc(time.time()-start_time)))\n",
    "\n",
    "sql_command = 'SELECT * FROM \"{schema}\".\"{table}\" WHERE ID = 15;'.format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "cur.execute('ROLLBACK;')\n",
    "print(\"Comment in table:\\n{comment}\\n\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "df['comment_language_code'] = pycld2.detect(str(df['comment']))[2][0][1]\n",
    "df['comment_language'] = pycld2.detect(str(df['comment']))[2][0][0].lower().replace('unknown','english')\n",
    "\n",
    "df\n",
    "#Since we are testing here we didn't commit to database so our changes are going to be rolled back after our session dies. It will be used after completing our test.\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original comment:\nIt's not your fault don't think that. Hey if this helps it does then if it doesn't well. At least I can talk about it.  I shot my dog. I had him for 10 years. I can home from partying and it was New Years so I was going to shoot off my .40 so...  Well I was loading it outside and I shot it off accidentally. Right into my dog. Me having about 5-7 tequila shots into me though if I go to sleep I would wake up and he would be okay. Well if I called a vet when I shot him I could have saved him. Or at least from his pain. So I woke up saw a dead dog in my yard… i cried for hours. That old dog still had a few good years in him.   That was my fault and you sitting on a little tiny mouse is more understandable than a man discharging a firearm into a shitzu. Don't feel to bad. \n\nComment after removing stopwords:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\nUpdated record(s) in 0 seconds\n\nComment in table:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                      date   subreddit      author author_flair_text  \\\n",
       "0  15 2015-05-01 00:00:00+00:00  offmychest  Zekkystyle                     \n",
       "\n",
       "   score                                            comment  \\\n",
       "0     14  fault think that. hey helps well. least talk i...   \n",
       "\n",
       "  comment_language_code comment_language  \n",
       "0                    en          english  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>subreddit</th>\n      <th>author</th>\n      <th>author_flair_text</th>\n      <th>score</th>\n      <th>comment</th>\n      <th>comment_language_code</th>\n      <th>comment_language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>2015-05-01 00:00:00+00:00</td>\n      <td>offmychest</td>\n      <td>Zekkystyle</td>\n      <td></td>\n      <td>14</td>\n      <td>fault think that. hey helps well. least talk i...</td>\n      <td>en</td>\n      <td>english</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "source": [
    "It reduces our data well and it still makes sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "source": [
    "## Sources:\n",
    "\n",
    " 1. [About Reddit](https://en.wikipedia.org/wiki/Reddit)\n",
    "\n",
    " 2. [Data source](https://www.kaggle.com/reddit/reddit-comments-may-2015/notebooks)\n",
    "\n",
    " 3. [Checking if a table exist with psycopg2 on postgreSQL](https://stackoverflow.com/questions/1874113/checking-if-a-postgresql-table-exists-under-python-and-probably-psycopg2)\n",
    "\n",
    " 4. [Using current time in UTC as default value in PostgreSQL. This is important because date is utc in the data](https://stackoverflow.com/questions/16609724/using-current-time-in-utc-as-default-value-in-postgresql)\n",
    "\n",
    " 5. [Creating multicolumn index on PostgreSQL](https://www.postgresql.org/docs/9.6/indexes-multicolumn.html)\n",
    "\n",
    " 6. [Checking if index exist](https://stackoverflow.com/questions/45983169/checking-for-existence-of-index-in-postgresql)\n",
    "\n",
    " 7. [How to execute start time and end time in python](https://www.codegrepper.com/code-examples/delphi/how+to+execute+from+start+time+to+end+time+in+python)\n",
    "\n",
    " 8. [Truncating numbers in python](https://www.w3schools.com/python/ref_math_trunc.asp)\n",
    "\n",
    " 9. [Removing stopwords](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe)\n",
    "\n",
    " 10. [Prevent SQL Injection in Python](https://realpython.com/prevent-python-sql-injection/)\n",
    "\n",
    " 11. [Preventing SQL Injection resulted errors but It needed to be done, data type conversation is the key here](https://stackoverflow.com/questions/39564755/programmingerror-psycopg2-programmingerror-cant-adapt-type-numpy-ndarray)\n",
    "\n",
    " 12. [About stopwords](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n",
    "\n",
    " 13. [How to detect language](https://github.com/aboSamoor/pycld2)\n",
    "\n",
    " 14. [Increasing timeout while installing new packages](https://stackoverflow.com/questions/43298872/how-to-solve-readtimeouterror-httpsconnectionpoolhost-pypi-python-org-port)\n",
    "\n",
    " 15. [PyCld2 is only works in linux systems](https://www.lfd.uci.edu/~gohlke/pythonlibs/)\n",
    "\n",
    " 16. [Replacing text to change unknown values to english](https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-column-of-a-pandas-dataframe)\n",
    "\n",
    " 17. [Electronic Products and Pricing Data](https://data.world/datafiniti/electronic-products-and-pricing-data/workspace/file?filename=DatafinitiElectronicsProductsPricingData.csv)\n",
    "\n",
    " 18. [Remove all commas between quotes](https://stackoverflow.com/questions/38336518/remove-all-commas-between-quotes)\n",
    "\n",
    " 19. [Check if file exist in directory](https://stackoverflow.com/questions/28144529/how-to-check-if-file-already-exists-if-not-download-on-python)\n",
    "\n",
    " 20. [Download files](https://stackabuse.com/download-files-with-python/)\n",
    "\n",
    " 21. [Deleting empty files](https://stackoverflow.com/questions/48046729/delete-empty-files)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}