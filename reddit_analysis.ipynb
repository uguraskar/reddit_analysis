{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0e8f02916ce4ec57490847cc8c54c12a9dae65680be9326a3269d9b7a61d6d19"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# About Dataset\n",
    "\n",
    "Reddit is a discussion website which users can post images and text in a subforum called subreddit which users can discuss about shared contents in comment section. This dataset contains 05/2015 comment submissions from reddit users with 54.504.410 rows and 22 columns.\n",
    "\n",
    "I got my data from kaggle unfornutely this dataset is too big to run on kaggle so I needed to download it.\n",
    "> https://www.kaggle.com/reddit/reddit-comments-may-2015/notebooks\n",
    "\n",
    "If you want a JSON format of this data you can download it from: https://files.pushshift.io/reddit/comments/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Accessing data from sqlite and cleaning it\n",
    "\n",
    "Used this sqlite query to clean the dataset before extracting it to csv because it caused problems while trying to import the data\n",
    "\n",
    "I didn't import authorflaircss_class field because it is not important for our analysis\n",
    "\n",
    "```sqlite\n",
    "create table reddit_2015_05 as\n",
    "select \n",
    "rd.created_utc,\n",
    "rd.ups,\n",
    "rd.subreddit_id,\n",
    "rd.link_id,\n",
    "rd.name,\n",
    "rd.score_hidden,\n",
    "replace(\n",
    "\treplace(\n",
    "\t\treplace(\n",
    "\t\t\t\treplace(\n",
    "\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\t\treplace(rd.author_flair_text,'\\','')\n",
    "\t\t\t\t\t\t,'*','')\n",
    "\t\t\t\t\t,'#','')\n",
    "\t\t\t\t, X'0A', ' ')\n",
    "\t\t,char(13),' ')\n",
    "\t,';','')\n",
    ",'\"','') as author_flair_text,\n",
    "rd.subreddit,\n",
    "rd.id,\n",
    "rd.removal_reason,\n",
    "rd.gilded,\n",
    "rd.downs,\n",
    "rd.archived,\n",
    "rd.author,\n",
    "rd.score,\n",
    "rd.retrieved_on,\n",
    "replace(\n",
    "\treplace(\n",
    "\t\treplace(\n",
    "\t\t\t\treplace(\n",
    "\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\treplace(\n",
    "\t\t\t\t\t\t\treplace(rd.body,'\\','')\n",
    "\t\t\t\t\t\t,'*','')\n",
    "\t\t\t\t\t,'#','')\n",
    "\t\t\t\t, X'0A', ' ')\n",
    "\t\t,char(13),' ')\n",
    "\t,';','')\n",
    ",'\"','') as body,\n",
    "rd.distinguished,\n",
    "rd.edited,\n",
    "rd.controversiality,\n",
    "rd.parent_id\n",
    "from may2015 rd;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Splitting csv data to make it ready for import\n",
    "\n",
    "I needed to split my csv file so I can import it to PostgreSQL because PostgreSQL copy command doesn't support files bigger than 4GB\n",
    "\n",
    "I used [csvsplitter](https://www.erdconcepts.com/dbtoolbox/csvsplitter/csvsplitter.zip) from [erdconcepts](https://www.erdconcepts.com/dbtoolbox.html)\n",
    "\n",
    "Opened up cmd and inserted these lines;\n",
    "\n",
    "```cmd\n",
    "cd C:\\data\\reddit\\csvsplitter\n",
    "\n",
    "CSVSplitter.exe filename=\"C:\\data\\reddit\\reddit_2015_05.csv\" rowcount=5000000\n",
    "```\n",
    "\n",
    "It spliced my csv to 11 files ranging from 1.2GB to 1.5GB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Creating table in PostgreSQL to import our dataset\n",
    "\n",
    "I created my PostgreSQL table with this query\n",
    "\n",
    "```PostgreSQL\n",
    "CREATE TABLE \"ODS\".\"EXT_REDDIT_COMMENTS\"\n",
    "(\n",
    "    created_utc integer,\n",
    "    ups integer,\n",
    "    subreddit_id text COLLATE pg_catalog.\"default\",\n",
    "    link_id text COLLATE pg_catalog.\"default\",\n",
    "    name text COLLATE pg_catalog.\"default\",\n",
    "    score_hidden text COLLATE pg_catalog.\"default\",\n",
    "    author_flair_text text COLLATE pg_catalog.\"default\",\n",
    "    subreddit text COLLATE pg_catalog.\"default\",\n",
    "    id text COLLATE pg_catalog.\"default\",\n",
    "    removal_reason text COLLATE pg_catalog.\"default\",\n",
    "    gilded integer,\n",
    "    downs integer,\n",
    "    archived text COLLATE pg_catalog.\"default\",\n",
    "    author text COLLATE pg_catalog.\"default\",\n",
    "    score integer,\n",
    "    retrieved_on integer,\n",
    "    body text COLLATE pg_catalog.\"default\",\n",
    "    distinguished text COLLATE pg_catalog.\"default\",\n",
    "    edited text COLLATE pg_catalog.\"default\",\n",
    "    controversiality integer,\n",
    "    parent_id text COLLATE pg_catalog.\"default\"\n",
    ")\n",
    "\n",
    "TABLESPACE pg_default;\n",
    "\n",
    "ALTER TABLE \"ODS\".\"EXT_REDDIT_COMMENTS\"\n",
    "    OWNER to postgres;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Importing dataset\n",
    "\n",
    "Then used PostgreSQL copy command to import my data;\n",
    "\n",
    "```PostgreSQL\n",
    "SET STATEMENT_TIMEOUT TO 3000000;\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-000.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-001.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-002.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-003.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-004.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-005.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-006.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-007.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-008.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-009.CSV' DELIMITER ';';\n",
    "\n",
    "COPY \"ODS\".\"EXT_REDDIT_COMMENTS\" FROM 'C:/data/reddit/REDDIT_2015_05-010.CSV' DELIMITER ';';\n",
    "\n",
    "COMMIT;\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Analyzing our data\n",
    "\n",
    "Original dataset is too big to handle(54.504.410 rows with 33.3GB size) maybe we should check if it is possible to reduce our data while not affecting our analysis\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT \n",
    "COUNT(*)                       \n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2;\n",
    "```\n",
    "This query reduces our data to 54.333.604 rows while removing comments like 'OK' which is not meaningful on its own.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT \n",
    "COUNT(*)                       \n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%');\n",
    "```\n",
    "This would remove 958 bot comments with comment author names contains \"-bot-\" or \"_bot_\", it is not that a huge decrease.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''',''))) LIKE '%im a bot%';\n",
    "```\n",
    "We could also filter comments with \"I'm a bot\" text, this also decreases dataset with 24.918 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]';\n",
    "```\n",
    "This query removes deleted comments which is 3.138.587 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.REMOVAL_REASON) = 0;\n",
    "```\n",
    "We should also remove removed comments which is replaced by removal reason instead of original comments.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %';\n",
    "```\n",
    "We should remove single word comments(1.885.966 rows) because they are not important for our analysis.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT (LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%'\n",
    "OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %'\n",
    "AND ERS.AUTHOR <> 'AutoModerator';\n",
    "```\n",
    "With this query we remove \"AutoModerator\" user which every subreddit uses it for moderation purposes, It filters 286.444 rows.\n",
    "\n",
    "```PostgreSQL\n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "WHERE 1=1\n",
    "AND LENGTH(ERS.BODY) > 2\n",
    "AND NOT(LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%' OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "AND ERS.BODY <> '[deleted]'\n",
    "AND LENGTH(ERS.removal_reason) = 0\n",
    "AND ERS.BODY LIKE '% %'\n",
    "AND ERS.AUTHOR <> 'AutoModerator'\n",
    "AND ERS.AUTHOR <> '[deleted]'\n",
    "```\n",
    "Filtering authors which they deleted their account removes 305.983 rows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Cleaning data\n",
    "\n",
    "Using sql analysis we found out which data to ignore, we must clean data before working on it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Table already exists\nIndex already exists\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import time\n",
    "import math\n",
    "\n",
    "conn_string = 'host={pghost} port={pgport} dbname={pgdatabase} user={pguser} password={pgpassword}'.format(pgdatabase='MEF-BDA-PROD',pguser='postgres',pgpassword='123',pghost='localhost',pgport='5432')\n",
    "conn=psycopg2.connect(conn_string)\n",
    "cur=conn.cursor()\n",
    "\n",
    "def check_if_table_exists(schema,table):\n",
    "    cur.execute(\"select exists(select * from information_schema.tables where table_schema='{schema}' AND table_name='{table}')\".format(schema=schema, table=table))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def check_if_index_exists(index):\n",
    "    cur.execute(\"SELECT EXISTS(SELECT * FROM PG_CLASS WHERE relname = '{index}')\".format(index=index))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "if(check_if_table_exists('EDW','DWH_REDDIT_COMMENTS')):\n",
    "    print('Table already exists')   \n",
    "else:\n",
    "    start_time = time.time()\n",
    "    cur.execute('set time zone UTC;')\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE \"EDW\".\"DWH_REDDIT_COMMENTS\" AS \n",
    "    SELECT\n",
    "    ROW_NUMBER() OVER (ORDER BY ERS.ID) AS ID,\n",
    "    TO_TIMESTAMP(GREATEST(ERS.CREATED_UTC ,CAST(ERS.EDITED AS INTEGER))) AS DATE,\n",
    "    ERS.SUBREDDIT,\n",
    "    ERS.AUTHOR,\n",
    "    ERS.AUTHOR_FLAIR_TEXT,\n",
    "    ERS.SCORE,\n",
    "    ERS.BODY AS COMMENT\n",
    "    FROM \"ODS\".\"EXT_REDDIT_COMMENTS\" ERS\n",
    "    WHERE 1=1\n",
    "    AND LENGTH(ERS.BODY) > 2\n",
    "    AND NOT(LOWER(ERS.AUTHOR) LIKE '%\\_bot\\_%' OR LOWER(ERS.AUTHOR) LIKE '%\\-bot\\-%')\n",
    "    AND NOT(LOWER(REPLACE(ERS.BODY,'''','')) LIKE '%im a bot%')\n",
    "    AND ERS.BODY <> '[deleted]'\n",
    "    AND LENGTH(ERS.removal_reason) = 0\n",
    "    AND ERS.BODY LIKE '% %'\n",
    "    AND ERS.AUTHOR <> 'AutoModerator'\n",
    "    AND ERS.AUTHOR <> '[deleted]';\n",
    "    \"\"\")\n",
    "    cur.execute('COMMIT;')\n",
    "    print(\"Table created in {execute_time} seconds\".format(execute_time=math.trunc(time.time()-start_time)))\n",
    "\n",
    "if(check_if_index_exists('IDX_DWH_REDDIT_COMMENTS#01')):\n",
    "    print('Index already exists')\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX \"IDX_DWH_REDDIT_COMMENTS#01\" \n",
    "    ON \"EDW\".\"DWH_REDDIT_COMMENTS\" USING BTREE(\n",
    "        \"id\" ASC NULLS LAST,\n",
    "        \"date\" ASC NULLS LAST\n",
    "    )\n",
    "    TABLESPACE PG_DEFAULT;\n",
    "    \"\"\")\n",
    "    cur.execute('COMMIT;')\n",
    "    print(\"Index created in {execute_time} seconds\".format(execute_time=math.trunc(time.time()-start_time)))"
   ]
  },
  {
   "source": [
    "1. We filtered our data and transformed epoch date to readable date and added numeric id to work our data with batch processing.\n",
    "    It reduced our row count 54.504.410(with 33.3GB) to 48.690.746(with 24.5GB) with 11% reduction in rows and 27% reduction in size.\n",
    "\n",
    "2. Added index to increase our read speed from table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                    key    max_price\n",
       "0                                000mah    93.374000\n",
       "1                               0g03674   209.851250\n",
       "2                              1000watt   998.326667\n",
       "3                                 1000x   191.996667\n",
       "4                              100400mm  1574.278571\n",
       "...                                 ...          ...\n",
       "2926                             zubehr    58.730000\n",
       "2927                              zubie    99.990000\n",
       "2928  zuxbbweveteztffywrybecztecqtezfbc  1799.970000\n",
       "2929                     zxqyvbwuwyydcq    18.158000\n",
       "2930      zxsqvcbvvywsrwuaasvczufystyyy    75.882857\n",
       "\n",
       "[2931 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>max_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000mah</td>\n      <td>93.374000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0g03674</td>\n      <td>209.851250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000watt</td>\n      <td>998.326667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000x</td>\n      <td>191.996667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100400mm</td>\n      <td>1574.278571</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2926</th>\n      <td>zubehr</td>\n      <td>58.730000</td>\n    </tr>\n    <tr>\n      <th>2927</th>\n      <td>zubie</td>\n      <td>99.990000</td>\n    </tr>\n    <tr>\n      <th>2928</th>\n      <td>zuxbbweveteztffywrybecztecqtezfbc</td>\n      <td>1799.970000</td>\n    </tr>\n    <tr>\n      <th>2929</th>\n      <td>zxqyvbwuwyydcq</td>\n      <td>18.158000</td>\n    </tr>\n    <tr>\n      <th>2930</th>\n      <td>zxsqvcbvvywsrwuaasvczufystyyy</td>\n      <td>75.882857</td>\n    </tr>\n  </tbody>\n</table>\n<p>2931 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import fsspec\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "from pandas import DataFrame\n",
    "\n",
    "def download_file_if_not_exists(file_url,file_name):\n",
    "    start_time = math.trunc(time.time())\n",
    "    if(os.path.exists(file_name) and os.stat(file_name).st_size==0):\n",
    "        os.remove(file_name)\n",
    "    if(not(os.path.exists(file_name))):\n",
    "        urllib.request.urlretrieve(file_url,file_name)\n",
    "        with open(file_name, 'r+', errors='ignore', encoding=\"utf-8\") as f:\n",
    "            file_text = f.read()\n",
    "            file_text = re.sub(r'\"[^\"]*\"', lambda m: m.group(0).replace(',', ' '), file_text).replace('\\\\','').replace('\"','').replace(\"'\",'')\n",
    "            f.seek(0)\n",
    "            f.write(file_text)\n",
    "            f.truncate()\n",
    "    end_time = math.trunc(time.time())\n",
    "    if(start_time!=end_time):\n",
    "        print(\"File downloaded and cleaned in {execute_time} seconds\".format(execute_time=end_time-start_time))\n",
    "\n",
    "file_name = \"DatafinitiElectronicsProductsPricingData.csv\"\n",
    "file_url = \"https://query.data.world/s/n7byb65oqj47oro2btcqqyas62zclv\"\n",
    "download_file_if_not_exists(file_url,file_name)\n",
    "\n",
    "file_name = \"electronic_products_pricing_df.xlsx\"\n",
    "if(os.path.exists(file_name)):\n",
    "    electronic_products_pricing_df = pd.read_excel(file_name, engine='openpyxl')\n",
    "else:\n",
    "    electronic_products_pricing_df = pd.read_csv(\"DatafinitiElectronicsProductsPricingData.csv\", encoding=\"utf-8\")\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.loc[:, ~electronic_products_pricing_df.columns.str.contains('^Unnamed')]\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df[electronic_products_pricing_df[\"prices.currency\"] == \"USD\"]\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df[[\"name\",\"brand\",\"categories\",\"prices.amountMax\"]]\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.groupby([\"name\",\"brand\",\"categories\"]).mean()\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.reset_index()\n",
    "    electronic_products_pricing_df = electronic_products_pricing_df.rename(columns = {\"prices.amountMax\":\"average_price\"})\n",
    "    electronic_products_pricing_df[\"keys\"] =  electronic_products_pricing_df[[\"name\",\"brand\",\"categories\"]].agg(' '.join, axis=1, ).str.lower()\n",
    "    electronic_products_pricing_df.to_excel(file_name, engine='xlsxwriter')\n",
    "\n",
    "file_name = \"keys_pricing_df.xlsx\"\n",
    "if(os.path.exists(file_name)):\n",
    "    keys_pricing_df = pd.read_excel(file_name, index_col=0, engine='openpyxl')\n",
    "else:\n",
    "    keys_pricing_df = DataFrame(electronic_products_pricing_df[\"keys\"].replace('&','').str.split(' ').tolist(), index=electronic_products_pricing_df[\"average_price\"]).stack()\n",
    "    keys_pricing_df = keys_pricing_df.reset_index()\n",
    "    keys_pricing_df.columns = [\"average_price\",\"level\",\"key\"]\n",
    "    keys_pricing_df = keys_pricing_df[keys_pricing_df[\"key\"] != \"&\"]\n",
    "    keys_pricing_df = keys_pricing_df[[\"key\",\"average_price\"]]\n",
    "    keys_pricing_df[\"key\"] = keys_pricing_df[\"key\"].str.replace(r'[^A-Za-z0-9]+','')\n",
    "    keys_pricing_df[\"key\"] = keys_pricing_df[\"key\"][~keys_pricing_df[\"key\"].str.isnumeric()]\n",
    "    keys_pricing_df = keys_pricing_df[keys_pricing_df[\"key\"] != \"\"] #Used this after realizing dropna doesn't work for some reason\n",
    "    keys_pricing_df = keys_pricing_df.dropna()\n",
    "    keys_pricing_df = keys_pricing_df.groupby([\"key\"]).max()\n",
    "    keys_pricing_df = keys_pricing_df.reset_index()\n",
    "    keys_pricing_df = keys_pricing_df.rename(columns = {\"average_price\":\"max_price\"})\n",
    "    keys_pricing_df.to_excel(file_name, engine='xlsxwriter')\n",
    "\n",
    "keys_pricing_df"
   ]
  },
  {
   "source": [
    "We are getting some price and key data to analize how much we would gain if we presented adds based on their comments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Removing stopwords and using Natural Language Processing while Detecting Language\n",
    "\n",
    "Stopwords are commonly used words that must be not stored because they do not give any significant value to analyze. We are going to remove them from our table so we can lower our datasize and proccess our data faster.\n",
    "\n",
    "We must test which libraries to use and how to use them;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "String 'Testing this value' returns: ('ENGLISH', 'en', 95, 1077.0)\n\nString 'Bu değeri deniyoruz' returns: ('TURKISH', 'tr', 95, 1706.0)\n\nString 'Test text' returns: ('Unknown', 'un', 0, 0.0)\n\n"
     ]
    }
   ],
   "source": [
    "import pycld2\n",
    "\n",
    "value = 'Testing this value'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0])) #This will return the language with highest confidence score.\n",
    "value = 'Bu değeri deniyoruz'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0]))\n",
    "value = 'Test text'\n",
    "print(\"String '{test_value}' returns: {most_confident_language_tuple}\\n\".format(test_value=value,most_confident_language_tuple=pycld2.detect(value)[2][0]))"
   ]
  },
  {
   "source": [
    "import nltk\n",
    "import pycld2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "sql_command = 'SELECT * FROM \"{schema}\".\"{table}\" WHERE ID = 15;'.format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "print(\"Original comment:\\n{comment}\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stop)]))\n",
    "\n",
    "print(\"Comment after removing stopwords:\\n{comment}\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "start_time = time.time()\n",
    "cur.execute(\"\"\"\n",
    "UPDATE \"EDW\".\"DWH_REDDIT_COMMENTS\"\n",
    "SET \"comment\" = %(comment)s\n",
    "WHERE \"id\" = %(id)s\n",
    "\"\"\", {'comment': str(df['comment'][0]), 'id': int(df['id'][0])})\n",
    "\n",
    "print(\"Updated record(s) in {execute_time} seconds\\n\".format(execute_time=math.trunc(time.time()-start_time)))\n",
    "\n",
    "sql_command = 'SELECT * FROM \"{schema}\".\"{table}\" WHERE ID = 15;'.format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "cur.execute('ROLLBACK;')\n",
    "print(\"Comment in table:\\n{comment}\\n\\n\".format(comment=df['comment'][0]))\n",
    "\n",
    "df['comment_language_code'] = pycld2.detect(str(df['comment']))[2][0][1]\n",
    "df['comment_language'] = pycld2.detect(str(df['comment']))[2][0][0].lower().replace('unknown','english')\n",
    "\n",
    "df\n",
    "#Since we are testing here we didn't commit to database so our changes are going to be rolled back after our session dies. It will be used after completing our test.\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 77,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original comment:\nIt's not your fault don't think that. Hey if this helps it does then if it doesn't well. At least I can talk about it.  I shot my dog. I had him for 10 years. I can home from partying and it was New Years so I was going to shoot off my .40 so...  Well I was loading it outside and I shot it off accidentally. Right into my dog. Me having about 5-7 tequila shots into me though if I go to sleep I would wake up and he would be okay. Well if I called a vet when I shot him I could have saved him. Or at least from his pain. So I woke up saw a dead dog in my yard… i cried for hours. That old dog still had a few good years in him.   That was my fault and you sitting on a little tiny mouse is more understandable than a man discharging a firearm into a shitzu. Don't feel to bad.\n\nComment after removing stopwords:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\nUpdated record(s) in 0 seconds\n\nComment in table:\nfault think that. hey helps well. least talk it. shot dog. 10 years. home partying new years going shoot .40 so... well loading outside shot accidentally. right dog. 5-7 tequila shots though go sleep would wake would okay. well called vet shot could saved him. least pain. woke saw dead dog yard… cried hours. old dog still good years him. fault sitting little tiny mouse understandable man discharging firearm shitzu. feel bad.\n\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                      date   subreddit      author author_flair_text  \\\n",
       "0  15 2015-05-01 00:00:00+00:00  offmychest  Zekkystyle                     \n",
       "\n",
       "   score                                            comment  \\\n",
       "0     14  fault think that. hey helps well. least talk i...   \n",
       "\n",
       "  comment_language_code comment_language  \n",
       "0                    en          english  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>subreddit</th>\n      <th>author</th>\n      <th>author_flair_text</th>\n      <th>score</th>\n      <th>comment</th>\n      <th>comment_language_code</th>\n      <th>comment_language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>2015-05-01 00:00:00+00:00</td>\n      <td>offmychest</td>\n      <td>Zekkystyle</td>\n      <td></td>\n      <td>14</td>\n      <td>fault think that. hey helps well. least talk i...</td>\n      <td>en</td>\n      <td>english</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ]
  },
  {
   "source": [
    "It reduces our data well and it still makes sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           id                      date             subreddit  \\\n",
       "0    30545495 2015-05-20 12:13:05+00:00                Turkey   \n",
       "1    30545496 2015-05-20 12:13:06+00:00                  cats   \n",
       "2    30545497 2015-05-20 12:13:06+00:00              Database   \n",
       "3    30545498 2015-05-20 12:13:06+00:00         unitedkingdom   \n",
       "4    30545499 2015-05-20 12:13:06+00:00                skyrim   \n",
       "..        ...                       ...                   ...   \n",
       "96   30545591 2015-05-20 12:13:12+00:00                Coffee   \n",
       "97   30545592 2015-05-20 12:13:12+00:00                  news   \n",
       "98   30545593 2015-05-20 12:13:12+00:00                  h1z1   \n",
       "99   30545594 2015-05-20 12:13:12+00:00  BeforeNAfterAdoption   \n",
       "100  30545595 2015-05-20 12:13:12+00:00               FlashTV   \n",
       "\n",
       "                  author         author_flair_text  score  \\\n",
       "0               shiguree                                1   \n",
       "1               lovechip                               10   \n",
       "2                cojajoc                                7   \n",
       "3                JamDunc  ex-Yorkshire now Sverige     11   \n",
       "4         Open_Info_of94                                2   \n",
       "..                   ...                       ...    ...   \n",
       "96    Italian_Not_Jewish                                1   \n",
       "97        PrivateHazzard                                1   \n",
       "98               Tenetri                                1   \n",
       "99            PantsGiver                               14   \n",
       "100  SuperBattleFranky37                                3   \n",
       "\n",
       "                                               comment comment_language_code  \\\n",
       "0    i nsana insan yaşama fırsatı verildiğinde şimd...                    tr   \n",
       "1    looks like cat used know  that sounds like wei...                    en   \n",
       "2    partitioning table would useful   partition da...                    en   \n",
       "3    well swedens system open run government   alth...                    en   \n",
       "4           lol said role play hard  thats real thing                     en   \n",
       "..                                                 ...                   ...   \n",
       "96   ive issue  brought french press help hard clea...                    en   \n",
       "97   logic incorrect    dont need war crime    real...                    en   \n",
       "98   but  ive gotten many streamers  lt5 viewers bl...                    en   \n",
       "99   inside cats really helps  also live fiv  cats ...                    en   \n",
       "100   im curious ciscos source dna tested crime scene                     en   \n",
       "\n",
       "    comment_language  word_count  \n",
       "0            turkish         127  \n",
       "1            english          24  \n",
       "2            english         111  \n",
       "3            english          40  \n",
       "4            english          12  \n",
       "..               ...         ...  \n",
       "96           english          96  \n",
       "97           english          19  \n",
       "98           english          37  \n",
       "99           english          32  \n",
       "100          english          16  \n",
       "\n",
       "[101 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>subreddit</th>\n      <th>author</th>\n      <th>author_flair_text</th>\n      <th>score</th>\n      <th>comment</th>\n      <th>comment_language_code</th>\n      <th>comment_language</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30545495</td>\n      <td>2015-05-20 12:13:05+00:00</td>\n      <td>Turkey</td>\n      <td>shiguree</td>\n      <td></td>\n      <td>1</td>\n      <td>i nsana insan yaşama fırsatı verildiğinde şimd...</td>\n      <td>tr</td>\n      <td>turkish</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30545496</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>cats</td>\n      <td>lovechip</td>\n      <td></td>\n      <td>10</td>\n      <td>looks like cat used know  that sounds like wei...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30545497</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>Database</td>\n      <td>cojajoc</td>\n      <td></td>\n      <td>7</td>\n      <td>partitioning table would useful   partition da...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>111</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30545498</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>unitedkingdom</td>\n      <td>JamDunc</td>\n      <td>ex-Yorkshire now Sverige</td>\n      <td>11</td>\n      <td>well swedens system open run government   alth...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30545499</td>\n      <td>2015-05-20 12:13:06+00:00</td>\n      <td>skyrim</td>\n      <td>Open_Info_of94</td>\n      <td></td>\n      <td>2</td>\n      <td>lol said role play hard  thats real thing</td>\n      <td>en</td>\n      <td>english</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>30545591</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>Coffee</td>\n      <td>Italian_Not_Jewish</td>\n      <td></td>\n      <td>1</td>\n      <td>ive issue  brought french press help hard clea...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>96</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>30545592</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>news</td>\n      <td>PrivateHazzard</td>\n      <td></td>\n      <td>1</td>\n      <td>logic incorrect    dont need war crime    real...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>30545593</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>h1z1</td>\n      <td>Tenetri</td>\n      <td></td>\n      <td>1</td>\n      <td>but  ive gotten many streamers  lt5 viewers bl...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>30545594</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>BeforeNAfterAdoption</td>\n      <td>PantsGiver</td>\n      <td></td>\n      <td>14</td>\n      <td>inside cats really helps  also live fiv  cats ...</td>\n      <td>en</td>\n      <td>english</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>30545595</td>\n      <td>2015-05-20 12:13:12+00:00</td>\n      <td>FlashTV</td>\n      <td>SuperBattleFranky37</td>\n      <td></td>\n      <td>3</td>\n      <td>im curious ciscos source dna tested crime scene</td>\n      <td>en</td>\n      <td>english</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>101 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import nltk\n",
    "import pycld2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "sql_command = \"\"\"SELECT id, date, subreddit, author, replace(author_flair_text,'''','') as author_flair_text, score, replace(COMMENT,'''','') as comment FROM \"{schema}\".\"{table}\" WHERE ID BETWEEN 30545495 AND 30545595;\"\"\".format(schema='EDW', table='DWH_REDDIT_COMMENTS')\n",
    "df = pd.read_sql(sql_command,conn)\n",
    "\n",
    "#30545495 AND 30545595\n",
    "\n",
    "df['comment_language_code'] = df['comment'].str.replace(r'[^A-Za-z0-9]+',' ').apply(pycld2.detect).apply(lambda x: x[2][0][1])\n",
    "df['comment_language'] = df['comment'].str.replace(r'[^A-Za-z0-9]+',' ').apply(pycld2.detect).apply(lambda x: x[2][0][0]).str.lower()\n",
    "\n",
    "def remove_stopwords(text,text_language):\n",
    "    text_without_stopwords = ' '\n",
    "    try:\n",
    "        text_without_stopwords = text_without_stopwords.join([word for word in text.lower().split() if word not in (stopwords.words(text_language))])\n",
    "    except:\n",
    "        text_without_stopwords = text_without_stopwords.join([word for word in text.lower().split() if word not in (stopwords.words('english'))])\n",
    "    \n",
    "    text_without_stopwords = re.sub(\"[\\W]\",\" \",text_without_stopwords,re.UNICODE)\n",
    "    #text_without_stopwords = re.sub(\"[^\\p{L} 0-9]\",\" \",text_without_stopwords,re.UNICODE)\n",
    "    return text_without_stopwords\n",
    "\n",
    "df['word_count'] = df['comment'].str.count(' ') #We are getting original word count without removing stopwords\n",
    "\n",
    "df['comment'] = df.apply(lambda x: remove_stopwords(x['comment'], x['comment_language']), axis=1)\n",
    "\n",
    "df['comment'] = df['comment'].str.replace(',','').replace(\"'\",\"\")\n",
    "df['author_flair_text'] = df['author_flair_text'].str.replace(',','').replace(\"'\",\"\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inserted rows between 1 and 10000\n",
      "Inserted rows between 10001 and 20000\n",
      "Inserted rows between 20001 and 30000\n",
      "Inserted rows between 30001 and 40000\n",
      "Inserted rows between 40001 and 50000\n",
      "Inserted rows between 50001 and 60000\n",
      "Inserted rows between 60001 and 70000\n",
      "Inserted rows between 70001 and 80000\n",
      "Inserted rows between 80001 and 90000\n",
      "Inserted rows between 90001 and 100000\n",
      "Inserted rows between 100001 and 110000\n",
      "Inserted rows between 110001 and 120000\n",
      "Inserted rows between 120001 and 130000\n",
      "Inserted rows between 130001 and 140000\n",
      "Inserted rows between 140001 and 150000\n",
      "Inserted rows between 150001 and 160000\n",
      "Inserted rows between 160001 and 170000\n",
      "Inserted rows between 170001 and 180000\n",
      "Inserted rows between 180001 and 190000\n",
      "Inserted rows between 190001 and 200000\n",
      "Inserted rows between 200001 and 210000\n",
      "Inserted rows between 210001 and 220000\n",
      "Inserted rows between 220001 and 230000\n",
      "Inserted rows between 230001 and 240000\n",
      "Inserted rows between 240001 and 250000\n",
      "Inserted rows between 250001 and 260000\n",
      "Inserted rows between 260001 and 270000\n",
      "Inserted rows between 270001 and 280000\n",
      "Inserted rows between 280001 and 290000\n",
      "Inserted rows between 290001 and 300000\n",
      "Inserted rows between 300001 and 310000\n",
      "Inserted rows between 310001 and 320000\n",
      "Inserted rows between 320001 and 330000\n",
      "Inserted rows between 330001 and 340000\n",
      "Inserted rows between 340001 and 350000\n",
      "Inserted rows between 350001 and 360000\n",
      "Inserted rows between 360001 and 370000\n",
      "Inserted rows between 370001 and 380000\n",
      "Inserted rows between 380001 and 390000\n",
      "Inserted rows between 390001 and 400000\n",
      "Inserted rows between 400001 and 410000\n",
      "Inserted rows between 410001 and 420000\n",
      "Inserted rows between 420001 and 430000\n",
      "Inserted rows between 430001 and 440000\n",
      "Inserted rows between 440001 and 450000\n",
      "Inserted rows between 450001 and 460000\n",
      "Inserted rows between 460001 and 470000\n",
      "Inserted rows between 470001 and 480000\n",
      "Inserted rows between 480001 and 490000\n",
      "Inserted rows between 490001 and 500000\n",
      "Inserted rows between 500001 and 510000\n",
      "Inserted rows between 510001 and 520000\n",
      "Inserted rows between 520001 and 530000\n",
      "Inserted rows between 530001 and 540000\n",
      "Inserted rows between 540001 and 550000\n",
      "Inserted rows between 550001 and 560000\n",
      "Inserted rows between 560001 and 570000\n",
      "Inserted rows between 570001 and 580000\n",
      "Inserted rows between 580001 and 590000\n",
      "Inserted rows between 590001 and 600000\n",
      "Inserted rows between 600001 and 610000\n",
      "Inserted rows between 610001 and 620000\n",
      "Inserted rows between 620001 and 630000\n",
      "Inserted rows between 630001 and 640000\n",
      "Inserted rows between 640001 and 650000\n",
      "Inserted rows between 650001 and 660000\n",
      "Inserted rows between 660001 and 670000\n",
      "Inserted rows between 670001 and 680000\n",
      "Inserted rows between 680001 and 690000\n",
      "Inserted rows between 690001 and 700000\n",
      "Inserted rows between 700001 and 710000\n",
      "Inserted rows between 710001 and 720000\n",
      "Inserted rows between 720001 and 730000\n",
      "Inserted rows between 730001 and 740000\n",
      "Inserted rows between 740001 and 750000\n",
      "Inserted rows between 750001 and 760000\n",
      "Inserted rows between 760001 and 770000\n",
      "Inserted rows between 770001 and 780000\n",
      "Inserted rows between 780001 and 790000\n",
      "Inserted rows between 790001 and 800000\n",
      "Inserted rows between 800001 and 810000\n",
      "Inserted rows between 810001 and 820000\n",
      "Inserted rows between 820001 and 830000\n",
      "Inserted rows between 830001 and 840000\n",
      "Inserted rows between 840001 and 850000\n",
      "Inserted rows between 850001 and 860000\n",
      "Inserted rows between 860001 and 870000\n",
      "Inserted rows between 870001 and 880000\n",
      "Inserted rows between 880001 and 890000\n",
      "Inserted rows between 890001 and 900000\n",
      "Inserted rows between 900001 and 910000\n",
      "Inserted rows between 910001 and 920000\n",
      "Inserted rows between 920001 and 930000\n",
      "Inserted rows between 930001 and 940000\n",
      "Inserted rows between 940001 and 950000\n",
      "Inserted rows between 950001 and 960000\n",
      "Inserted rows between 960001 and 970000\n",
      "Inserted rows between 970001 and 980000\n",
      "Inserted rows between 980001 and 990000\n",
      "Inserted rows between 990001 and 1000000\n",
      "Inserted rows between 1000001 and 1010000\n",
      "Inserted rows between 1010001 and 1020000\n",
      "Inserted rows between 1020001 and 1030000\n",
      "Inserted rows between 1030001 and 1040000\n",
      "Inserted rows between 1040001 and 1050000\n",
      "Inserted rows between 1050001 and 1060000\n",
      "Inserted rows between 1060001 and 1070000\n",
      "Inserted rows between 1070001 and 1080000\n",
      "Inserted rows between 1080001 and 1090000\n",
      "Inserted rows between 1090001 and 1100000\n",
      "Inserted rows between 1100001 and 1110000\n",
      "Inserted rows between 1110001 and 1120000\n",
      "Inserted rows between 1120001 and 1130000\n",
      "Inserted rows between 1130001 and 1140000\n",
      "Inserted rows between 1140001 and 1150000\n",
      "Inserted rows between 1150001 and 1160000\n",
      "Inserted rows between 1160001 and 1170000\n",
      "Inserted rows between 1170001 and 1180000\n",
      "Inserted rows between 1180001 and 1190000\n",
      "Inserted rows between 1190001 and 1200000\n",
      "Inserted rows between 1200001 and 1210000\n",
      "Inserted rows between 1210001 and 1220000\n",
      "Inserted rows between 1220001 and 1230000\n",
      "Inserted rows between 1230001 and 1240000\n",
      "Inserted rows between 1240001 and 1250000\n",
      "Inserted rows between 1250001 and 1260000\n",
      "Inserted rows between 1260001 and 1270000\n",
      "Inserted rows between 1270001 and 1280000\n",
      "Inserted rows between 1280001 and 1290000\n",
      "Inserted rows between 1290001 and 1300000\n",
      "Inserted rows between 1300001 and 1310000\n",
      "Inserted rows between 1310001 and 1320000\n",
      "Inserted rows between 1320001 and 1330000\n",
      "Inserted rows between 1330001 and 1340000\n",
      "Inserted rows between 1340001 and 1350000\n",
      "Inserted rows between 1350001 and 1360000\n",
      "Inserted rows between 1360001 and 1370000\n",
      "Inserted rows between 1370001 and 1380000\n",
      "Inserted rows between 1380001 and 1390000\n",
      "Inserted rows between 1390001 and 1400000\n",
      "Inserted rows between 1400001 and 1410000\n",
      "Inserted rows between 1410001 and 1420000\n",
      "Inserted rows between 1420001 and 1430000\n",
      "Inserted rows between 1430001 and 1440000\n",
      "Inserted rows between 1440001 and 1450000\n",
      "Inserted rows between 1450001 and 1460000\n",
      "Inserted rows between 1460001 and 1470000\n",
      "Inserted rows between 1470001 and 1480000\n",
      "Inserted rows between 1480001 and 1490000\n",
      "Inserted rows between 1490001 and 1500000\n",
      "Inserted rows between 1500001 and 1510000\n",
      "Inserted rows between 1510001 and 1520000\n",
      "Inserted rows between 1520001 and 1530000\n",
      "Inserted rows between 1530001 and 1540000\n",
      "Inserted rows between 1540001 and 1550000\n",
      "Inserted rows between 1550001 and 1560000\n",
      "Inserted rows between 1560001 and 1570000\n",
      "Inserted rows between 1570001 and 1580000\n",
      "Inserted rows between 1580001 and 1590000\n",
      "Inserted rows between 1590001 and 1600000\n",
      "Inserted rows between 1600001 and 1610000\n",
      "Inserted rows between 1610001 and 1620000\n",
      "Inserted rows between 1620001 and 1630000\n",
      "Inserted rows between 1630001 and 1640000\n",
      "Inserted rows between 1640001 and 1650000\n",
      "Inserted rows between 1650001 and 1660000\n",
      "Inserted rows between 1660001 and 1670000\n",
      "Inserted rows between 1670001 and 1680000\n",
      "Inserted rows between 1680001 and 1690000\n",
      "Inserted rows between 1690001 and 1700000\n",
      "Inserted rows between 1700001 and 1710000\n",
      "Inserted rows between 1710001 and 1720000\n",
      "Inserted rows between 1720001 and 1730000\n",
      "Inserted rows between 1730001 and 1740000\n",
      "Inserted rows between 1740001 and 1750000\n",
      "Inserted rows between 1750001 and 1760000\n",
      "Inserted rows between 1760001 and 1770000\n",
      "Inserted rows between 1770001 and 1780000\n",
      "Inserted rows between 1780001 and 1790000\n",
      "Inserted rows between 1790001 and 1800000\n",
      "Inserted rows between 1800001 and 1810000\n",
      "Inserted rows between 1810001 and 1820000\n",
      "Inserted rows between 1820001 and 1830000\n",
      "Inserted rows between 1830001 and 1840000\n",
      "Inserted rows between 1840001 and 1850000\n",
      "Inserted rows between 1850001 and 1860000\n",
      "Inserted rows between 1860001 and 1870000\n",
      "Inserted rows between 1870001 and 1880000\n",
      "Inserted rows between 1880001 and 1890000\n",
      "Inserted rows between 1890001 and 1900000\n",
      "Inserted rows between 1900001 and 1910000\n",
      "Inserted rows between 1910001 and 1920000\n",
      "Inserted rows between 1920001 and 1930000\n",
      "Inserted rows between 1930001 and 1940000\n",
      "Inserted rows between 1940001 and 1950000\n",
      "Inserted rows between 1950001 and 1960000\n",
      "Inserted rows between 1960001 and 1970000\n",
      "Inserted rows between 1970001 and 1980000\n",
      "Inserted rows between 1980001 and 1990000\n",
      "Inserted rows between 1990001 and 2000000\n",
      "Inserted rows between 2000001 and 2010000\n",
      "Inserted rows between 2010001 and 2020000\n",
      "Inserted rows between 2020001 and 2030000\n",
      "Inserted rows between 2030001 and 2040000\n",
      "Inserted rows between 2040001 and 2050000\n",
      "Inserted rows between 2050001 and 2060000\n",
      "Inserted rows between 2060001 and 2070000\n",
      "Inserted rows between 2070001 and 2080000\n",
      "Inserted rows between 2080001 and 2090000\n",
      "Inserted rows between 2090001 and 2100000\n",
      "Inserted rows between 2100001 and 2110000\n",
      "Inserted rows between 2110001 and 2120000\n",
      "Inserted rows between 2120001 and 2130000\n",
      "Inserted rows between 2130001 and 2140000\n",
      "Inserted rows between 2140001 and 2150000\n",
      "Inserted rows between 2150001 and 2160000\n",
      "Inserted rows between 2160001 and 2170000\n",
      "Inserted rows between 2170001 and 2180000\n",
      "Inserted rows between 2180001 and 2190000\n",
      "Inserted rows between 2190001 and 2200000\n",
      "Inserted rows between 2200001 and 2210000\n",
      "Inserted rows between 2210001 and 2220000\n",
      "Inserted rows between 2220001 and 2230000\n",
      "Inserted rows between 2230001 and 2240000\n",
      "Inserted rows between 2240001 and 2250000\n",
      "Inserted rows between 2250001 and 2260000\n",
      "Inserted rows between 2260001 and 2270000\n",
      "Inserted rows between 2270001 and 2280000\n",
      "Inserted rows between 2280001 and 2290000\n",
      "Inserted rows between 2290001 and 2300000\n",
      "Inserted rows between 2300001 and 2310000\n",
      "Inserted rows between 2310001 and 2320000\n",
      "Inserted rows between 2320001 and 2330000\n",
      "Inserted rows between 2330001 and 2340000\n",
      "Inserted rows between 2340001 and 2350000\n",
      "Inserted rows between 2350001 and 2360000\n",
      "Inserted rows between 2360001 and 2370000\n",
      "Inserted rows between 2370001 and 2380000\n",
      "Inserted rows between 2380001 and 2390000\n",
      "Inserted rows between 2390001 and 2400000\n",
      "Inserted rows between 2400001 and 2410000\n",
      "Inserted rows between 2410001 and 2420000\n",
      "Inserted rows between 2420001 and 2430000\n",
      "Inserted rows between 2430001 and 2440000\n",
      "Inserted rows between 2440001 and 2450000\n",
      "Inserted rows between 2450001 and 2460000\n",
      "Inserted rows between 2460001 and 2470000\n",
      "Inserted rows between 2470001 and 2480000\n",
      "Inserted rows between 2480001 and 2490000\n",
      "Inserted rows between 2490001 and 2500000\n",
      "Inserted rows between 2500001 and 2510000\n",
      "Inserted rows between 2510001 and 2520000\n",
      "Inserted rows between 2520001 and 2530000\n",
      "Inserted rows between 2530001 and 2540000\n",
      "Inserted rows between 2540001 and 2550000\n",
      "Inserted rows between 2550001 and 2560000\n",
      "Inserted rows between 2560001 and 2570000\n",
      "Inserted rows between 2570001 and 2580000\n",
      "Inserted rows between 2580001 and 2590000\n",
      "Inserted rows between 2590001 and 2600000\n",
      "Inserted rows between 2600001 and 2610000\n",
      "Inserted rows between 2610001 and 2620000\n",
      "Inserted rows between 2620001 and 2630000\n",
      "Inserted rows between 2630001 and 2640000\n",
      "Inserted rows between 2640001 and 2650000\n",
      "Inserted rows between 2650001 and 2660000\n",
      "Inserted rows between 2660001 and 2670000\n",
      "Inserted rows between 2670001 and 2680000\n",
      "Inserted rows between 2680001 and 2690000\n",
      "Inserted rows between 2690001 and 2700000\n",
      "Inserted rows between 2700001 and 2710000\n",
      "Inserted rows between 2710001 and 2720000\n",
      "Inserted rows between 2720001 and 2730000\n",
      "Inserted rows between 2730001 and 2740000\n",
      "Inserted rows between 2740001 and 2750000\n",
      "Inserted rows between 2750001 and 2760000\n",
      "Inserted rows between 2760001 and 2770000\n",
      "Inserted rows between 2770001 and 2780000\n",
      "Inserted rows between 2780001 and 2790000\n",
      "Inserted rows between 2790001 and 2800000\n",
      "Inserted rows between 2800001 and 2810000\n",
      "Inserted rows between 2810001 and 2820000\n",
      "Inserted rows between 2820001 and 2830000\n",
      "Inserted rows between 2830001 and 2840000\n",
      "Inserted rows between 2840001 and 2850000\n",
      "Inserted rows between 2850001 and 2860000\n",
      "Inserted rows between 2860001 and 2870000\n",
      "Inserted rows between 2870001 and 2880000\n",
      "Inserted rows between 2880001 and 2890000\n",
      "Inserted rows between 2890001 and 2900000\n",
      "Inserted rows between 2900001 and 2910000\n",
      "Inserted rows between 2910001 and 2920000\n",
      "Inserted rows between 2920001 and 2930000\n",
      "Inserted rows between 2930001 and 2940000\n",
      "Inserted rows between 2940001 and 2950000\n",
      "Inserted rows between 2950001 and 2960000\n",
      "Inserted rows between 2960001 and 2970000\n",
      "Inserted rows between 2970001 and 2980000\n",
      "Inserted rows between 2980001 and 2990000\n",
      "Inserted rows between 2990001 and 3000000\n",
      "Inserted rows between 3000001 and 3010000\n",
      "Inserted rows between 3010001 and 3020000\n",
      "Inserted rows between 3020001 and 3030000\n",
      "Inserted rows between 3030001 and 3040000\n",
      "Inserted rows between 3040001 and 3050000\n",
      "Inserted rows between 3050001 and 3060000\n",
      "Inserted rows between 3060001 and 3070000\n",
      "Inserted rows between 3070001 and 3080000\n",
      "Inserted rows between 3080001 and 3090000\n",
      "Inserted rows between 3090001 and 3100000\n",
      "Inserted rows between 3100001 and 3110000\n",
      "Inserted rows between 3110001 and 3120000\n",
      "Inserted rows between 3120001 and 3130000\n",
      "Inserted rows between 3130001 and 3140000\n",
      "Inserted rows between 3140001 and 3150000\n",
      "Inserted rows between 3150001 and 3160000\n",
      "Inserted rows between 3160001 and 3170000\n",
      "Inserted rows between 3170001 and 3180000\n",
      "Inserted rows between 3180001 and 3190000\n",
      "Inserted rows between 3190001 and 3200000\n",
      "Inserted rows between 3200001 and 3210000\n",
      "Inserted rows between 3210001 and 3220000\n",
      "Inserted rows between 3220001 and 3230000\n",
      "Inserted rows between 3230001 and 3240000\n",
      "Inserted rows between 3240001 and 3250000\n",
      "Inserted rows between 3250001 and 3260000\n",
      "Inserted rows between 3260001 and 3270000\n",
      "Inserted rows between 3270001 and 3280000\n",
      "Inserted rows between 3280001 and 3290000\n",
      "Inserted rows between 3290001 and 3300000\n",
      "Inserted rows between 3300001 and 3310000\n",
      "Inserted rows between 3310001 and 3320000\n",
      "Inserted rows between 3320001 and 3330000\n",
      "Inserted rows between 3330001 and 3340000\n",
      "Inserted rows between 3340001 and 3350000\n",
      "Inserted rows between 3350001 and 3360000\n",
      "Inserted rows between 3360001 and 3370000\n",
      "Inserted rows between 3370001 and 3380000\n",
      "Inserted rows between 3380001 and 3390000\n",
      "Inserted rows between 3390001 and 3400000\n",
      "Inserted rows between 3400001 and 3410000\n",
      "Inserted rows between 3410001 and 3420000\n",
      "Inserted rows between 3420001 and 3430000\n",
      "Inserted rows between 3430001 and 3440000\n",
      "Inserted rows between 3440001 and 3450000\n",
      "Inserted rows between 3450001 and 3460000\n",
      "Inserted rows between 3460001 and 3470000\n",
      "Inserted rows between 3470001 and 3480000\n",
      "Inserted rows between 3480001 and 3490000\n",
      "Inserted rows between 3490001 and 3500000\n",
      "Inserted rows between 3500001 and 3510000\n",
      "Inserted rows between 3510001 and 3520000\n",
      "Inserted rows between 3520001 and 3530000\n",
      "Inserted rows between 3530001 and 3540000\n",
      "Inserted rows between 3540001 and 3550000\n",
      "Inserted rows between 3550001 and 3560000\n",
      "Inserted rows between 3560001 and 3570000\n",
      "Inserted rows between 3570001 and 3580000\n",
      "Inserted rows between 3580001 and 3590000\n",
      "Inserted rows between 3590001 and 3600000\n",
      "Inserted rows between 3600001 and 3610000\n",
      "Inserted rows between 3610001 and 3620000\n",
      "Inserted rows between 3620001 and 3630000\n",
      "Inserted rows between 3630001 and 3640000\n",
      "Inserted rows between 3640001 and 3650000\n",
      "Inserted rows between 3650001 and 3660000\n",
      "Inserted rows between 3660001 and 3670000\n",
      "Inserted rows between 3670001 and 3680000\n",
      "Inserted rows between 3680001 and 3690000\n",
      "Inserted rows between 3690001 and 3700000\n",
      "Inserted rows between 3700001 and 3710000\n",
      "Inserted rows between 3710001 and 3720000\n",
      "Inserted rows between 3720001 and 3730000\n",
      "Inserted rows between 3730001 and 3740000\n",
      "Inserted rows between 3740001 and 3750000\n",
      "Inserted rows between 3750001 and 3760000\n",
      "Inserted rows between 3760001 and 3770000\n",
      "Inserted rows between 3770001 and 3780000\n",
      "Inserted rows between 3780001 and 3790000\n",
      "Inserted rows between 3790001 and 3800000\n",
      "Inserted rows between 3800001 and 3810000\n",
      "Inserted rows between 3810001 and 3820000\n",
      "Inserted rows between 3820001 and 3830000\n",
      "Inserted rows between 3830001 and 3840000\n",
      "Inserted rows between 3840001 and 3850000\n",
      "Inserted rows between 3850001 and 3860000\n",
      "Inserted rows between 3860001 and 3870000\n",
      "Inserted rows between 3870001 and 3880000\n",
      "Inserted rows between 3880001 and 3890000\n",
      "Inserted rows between 3890001 and 3900000\n",
      "Inserted rows between 3900001 and 3910000\n",
      "Inserted rows between 3910001 and 3920000\n",
      "Inserted rows between 3920001 and 3930000\n",
      "Inserted rows between 3930001 and 3940000\n",
      "Inserted rows between 3940001 and 3950000\n",
      "Inserted rows between 3950001 and 3960000\n",
      "Inserted rows between 3960001 and 3970000\n",
      "Inserted rows between 3970001 and 3980000\n",
      "Inserted rows between 3980001 and 3990000\n",
      "Inserted rows between 3990001 and 4000000\n",
      "Inserted rows between 4000001 and 4010000\n",
      "Inserted rows between 4010001 and 4020000\n",
      "Inserted rows between 4020001 and 4030000\n",
      "Inserted rows between 4030001 and 4040000\n",
      "Inserted rows between 4040001 and 4050000\n",
      "Inserted rows between 4050001 and 4060000\n",
      "Inserted rows between 4060001 and 4070000\n",
      "Inserted rows between 4070001 and 4080000\n",
      "Inserted rows between 4080001 and 4090000\n",
      "Inserted rows between 4090001 and 4100000\n",
      "Inserted rows between 4100001 and 4110000\n",
      "Inserted rows between 4110001 and 4120000\n",
      "Inserted rows between 4120001 and 4130000\n",
      "Inserted rows between 4130001 and 4140000\n",
      "Inserted rows between 4140001 and 4150000\n",
      "Inserted rows between 4150001 and 4160000\n",
      "Inserted rows between 4160001 and 4170000\n",
      "Inserted rows between 4170001 and 4180000\n",
      "Inserted rows between 4180001 and 4190000\n",
      "Inserted rows between 4190001 and 4200000\n",
      "Inserted rows between 4200001 and 4210000\n",
      "Inserted rows between 4210001 and 4220000\n",
      "Inserted rows between 4220001 and 4230000\n",
      "Inserted rows between 4230001 and 4240000\n",
      "Inserted rows between 4240001 and 4250000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import psycopg2.extras as extras\n",
    "from io import StringIO\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def df_column_conversation(df, column_name, type):\n",
    "    if(type == 'timestamp'):\n",
    "        df[column_name] = df[column_name].apply(lambda x: f\"'{x}'::timestamp\")\n",
    "    if(type == 'text'):\n",
    "        df[column_name] = df[column_name].apply(lambda x: f\"'{x}'\")\n",
    "\n",
    "def remove_stopwords(text,text_language):\n",
    "    text_without_stopwords = ' '\n",
    "    try:\n",
    "        text_without_stopwords = text_without_stopwords.join([word for word in text.lower().split() if word not in (stopwords.words(text_language))])\n",
    "    except:\n",
    "        text_without_stopwords = text_without_stopwords.join([word for word in text.lower().split() if word not in (stopwords.words('english'))])\n",
    "    \n",
    "    text_without_stopwords = re.sub(\"[\\W]\",\" \",text_without_stopwords,re.UNICODE)\n",
    "    return text_without_stopwords\n",
    "\n",
    "def execute_mogrify(conn, df, schema, table):\n",
    "    \"\"\"\n",
    "    Using cursor.mogrify() to build the bulk insert query\n",
    "    then cursor.execute() to execute the query\n",
    "    \"\"\"\n",
    "    # Create a list of tupples from the dataframe values\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(list(df.columns))\n",
    "    # SQL quert to execute\n",
    "    cursor = conn.cursor()    \n",
    "    try:\n",
    "        for tup in tuples:\n",
    "            query  = \"\"\"INSERT INTO \"{schema}\".\"{table}\"({cols}) VALUES ({values})\"\"\".format(schema=schema,table=table, cols=cols, values=\",\".join(map(str,tup)))\n",
    "            cursor.execute(query)\n",
    "            conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    #print(\"execute_mogrify() done\")\n",
    "    cursor.close()\n",
    "\n",
    "cur.execute('SELECT MAX(ID) FROM \"EDW\".\"DWH_REDDIT_COMMENTS_DETAIL\"')\n",
    "start_row = cur.fetchone()[0]\n",
    "row_per_loop = 10000\n",
    "cur.execute('SELECT COUNT(1) FROM \"EDW\".\"DWH_REDDIT_COMMENTS\"')\n",
    "end_row = cur.fetchone()[0]\n",
    "\n",
    "start_time = math.trunc(time.time())\n",
    "for i in range(start_row,end_row,row_per_loop):\n",
    "    sql_command = \"\"\"SELECT id, date, subreddit, author, replace(author_flair_text,'''','') as author_flair_text, score, replace(COMMENT,'''','') as comment FROM \"{schema}\".\"{table}\" WHERE ID BETWEEN {start_row} AND {end_row};\"\"\".format(schema='EDW', table='DWH_REDDIT_COMMENTS', start_row=i, end_row=i+row_per_loop-1)\n",
    "    df = pd.read_sql(sql_command,conn)\n",
    "    df['comment_language_code'] = df['comment'].str.replace(r'[^A-Za-z0-9]+',' ').apply(pycld2.detect).apply(lambda x: x[2][0][1])\n",
    "    df['comment_language'] = df['comment'].str.replace(r'[^A-Za-z0-9]+',' ').apply(pycld2.detect).apply(lambda x: x[2][0][0]).str.lower()\n",
    "    df['word_count'] = df['comment'].str.count(' ') #We are getting original word count without removing stopwords\n",
    "    df['comment'] = df.apply(lambda x: remove_stopwords(x['comment'], x['comment_language']), axis=1)\n",
    "    df['comment'] = df['comment'].str.replace(',','')\n",
    "    df['author_flair_text'] = df['author_flair_text'].str.replace(',','')\n",
    "    df_column_conversation(df, 'date', 'timestamp')\n",
    "    df_column_conversation(df, 'subreddit', 'text')\n",
    "    df_column_conversation(df, 'author', 'text')\n",
    "    df_column_conversation(df, 'author_flair_text', 'text')\n",
    "    df_column_conversation(df, 'comment', 'text')\n",
    "    df_column_conversation(df, 'comment_language_code', 'text')\n",
    "    df_column_conversation(df, 'comment_language', 'text')\n",
    "    execute_mogrify(conn,df,\"EDW\",\"DWH_REDDIT_COMMENTS_DETAIL\")\n",
    "    print(\"Inserted rows between {start_row} and {end_row}\".format(start_row=i, end_row=i+row_per_loop-1))\n",
    "\n",
    "end_time = math.trunc(time.time())\n",
    "print(\"Data transformation completed in {execute_time} seconds.\".format(execute_time=end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "source": [
    "# Sources:\n",
    "\n",
    " 1. [About Reddit](https://en.wikipedia.org/wiki/Reddit)\n",
    "\n",
    " 2. [Data source](https://www.kaggle.com/reddit/reddit-comments-may-2015/notebooks)\n",
    "\n",
    " 3. [Checking if a table exist with psycopg2 on postgreSQL](https://stackoverflow.com/questions/1874113/checking-if-a-postgresql-table-exists-under-python-and-probably-psycopg2)\n",
    "\n",
    " 4. [Using current time in UTC as default value in PostgreSQL. This is important because date is utc in the data](https://stackoverflow.com/questions/16609724/using-current-time-in-utc-as-default-value-in-postgresql)\n",
    "\n",
    " 5. [Creating multicolumn index on PostgreSQL](https://www.postgresql.org/docs/9.6/indexes-multicolumn.html)\n",
    "\n",
    " 6. [Checking if index exist](https://stackoverflow.com/questions/45983169/checking-for-existence-of-index-in-postgresql)\n",
    "\n",
    " 7. [How to execute start time and end time in python](https://www.codegrepper.com/code-examples/delphi/how+to+execute+from+start+time+to+end+time+in+python)\n",
    "\n",
    " 8. [Truncating numbers in python](https://www.w3schools.com/python/ref_math_trunc.asp)\n",
    "\n",
    " 9. [Removing stopwords](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe)\n",
    "\n",
    " 10. [Prevent SQL Injection in Python](https://realpython.com/prevent-python-sql-injection/)\n",
    "\n",
    " 11. [Preventing SQL Injection resulted errors but It needed to be done, data type conversation is the key here](https://stackoverflow.com/questions/39564755/programmingerror-psycopg2-programmingerror-cant-adapt-type-numpy-ndarray)\n",
    "\n",
    " 12. [About stopwords](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n",
    "\n",
    " 13. [How to detect language](https://github.com/aboSamoor/pycld2)\n",
    "\n",
    " 14. [Increasing timeout while installing new packages](https://stackoverflow.com/questions/43298872/how-to-solve-readtimeouterror-httpsconnectionpoolhost-pypi-python-org-port)\n",
    "\n",
    " 15. [PyCld2 is only works in linux systems](https://www.lfd.uci.edu/~gohlke/pythonlibs/)\n",
    "\n",
    " 16. [Replacing text to change unknown values to english](https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-column-of-a-pandas-dataframe)\n",
    "\n",
    " 17. [Electronic Products and Pricing Data](https://data.world/datafiniti/electronic-products-and-pricing-data/workspace/file?filename=DatafinitiElectronicsProductsPricingData.csv)\n",
    "\n",
    " 18. [Remove all commas between quotes](https://stackoverflow.com/questions/38336518/remove-all-commas-between-quotes)\n",
    "\n",
    " 19. [Check if file exist in directory](https://stackoverflow.com/questions/28144529/how-to-check-if-file-already-exists-if-not-download-on-python)\n",
    "\n",
    " 20. [Download files](https://stackabuse.com/download-files-with-python/)\n",
    "\n",
    " 21. [Deleting empty files](https://stackoverflow.com/questions/48046729/delete-empty-files)\n",
    "\n",
    " 22. [Removing unnamed columns](https://stackoverflow.com/questions/43983622/remove-unnamed-columns-in-pandas-dataframe)\n",
    "\n",
    " 23. [Split (explode) pandas dataframe string entry to separate rows](https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows)\n",
    "\n",
    " 24. [Dropping numeric values](https://stackoverflow.com/questions/48636170/dropping-numeric-rows-from-dataframe-in-python)\n",
    "\n",
    " 25. [Join function](https://www.geeksforgeeks.org/join-function-python/)\n",
    "\n",
    " 26. [Apply function with two arguments to columns](https://stackoverflow.com/questions/34279378/python-pandas-apply-function-with-two-arguments-to-columns)\n",
    "\n",
    " 27. [Pandas to PostgreSQL using Psycopg2: Bulk Insert Performance Benchmark](https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}